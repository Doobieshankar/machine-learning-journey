{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19069598-431e-4fbe-9adb-6e7830a5ec18",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Syllabus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bb72ca-c806-4183-865a-795e126b94e9",
   "metadata": {},
   "source": [
    "Let's start with the topics we gonna cover in this 30 Days of Data Science Series,\n",
    "\n",
    "We will primarily focus on learning Data Science and Machine Learning Algorithms \n",
    "\n",
    "Day 1: Linear Regression\n",
    "- Concept: Predict continuous values.\n",
    "- Implementation: Ordinary Least Squares.\n",
    "- Evaluation: R-squared, RMSE.\n",
    "\n",
    "Day 2: Logistic Regression\n",
    "- Concept: Binary classification.\n",
    "- Implementation: Sigmoid function.\n",
    "- Evaluation: Confusion matrix, ROC-AUC.\n",
    "\n",
    "Day 3: Decision Trees\n",
    "- Concept: Tree-based model for classification/regression.\n",
    "- Implementation: Recursive splitting.\n",
    "- Evaluation: Accuracy, Gini impurity.\n",
    "\n",
    "Day 4: Random Forest\n",
    "- Concept: Ensemble of decision trees.\n",
    "- Implementation: Bagging.\n",
    "- Evaluation: Out-of-bag error, feature importance.\n",
    "\n",
    "Day 5: Gradient Boosting\n",
    "- Concept: Sequential ensemble method.\n",
    "- Implementation: Boosting.\n",
    "- Evaluation: Learning rate, number of estimators.\n",
    "\n",
    "Day 6: Support Vector Machines (SVM)\n",
    "- Concept: Classification using hyperplanes.\n",
    "- Implementation: Kernel trick.\n",
    "- Evaluation: Margin maximization, support vectors.\n",
    "\n",
    "Day 7: k-Nearest Neighbors (k-NN)\n",
    "- Concept: Instance-based learning.\n",
    "- Implementation: Distance metrics.\n",
    "- Evaluation: k-value tuning, distance functions.\n",
    "\n",
    "Day 8: Naive Bayes\n",
    "- Concept: Probabilistic classifier.\n",
    "- Implementation: Bayes' theorem.\n",
    "- Evaluation: Prior probabilities, likelihood.\n",
    "\n",
    "Day 9: k-Means Clustering\n",
    "- Concept: Partitioning data into k clusters.\n",
    "- Implementation: Centroid initialization.\n",
    "- Evaluation: Inertia, silhouette score.\n",
    "\n",
    "Day 10: Hierarchical Clustering\n",
    "- Concept: Nested clusters.\n",
    "- Implementation: Agglomerative method.\n",
    "- Evaluation: Dendrograms, linkage methods.\n",
    "\n",
    "Day 11: Principal Component Analysis (PCA)\n",
    "- Concept: Dimensionality reduction.\n",
    "- Implementation: Eigenvectors, eigenvalues.\n",
    "- Evaluation: Explained variance.\n",
    "\n",
    "Day 12: Association Rule Learning\n",
    "- Concept: Discover relationships between variables.\n",
    "- Implementation: Apriori algorithm.\n",
    "- Evaluation: Support, confidence, lift.\n",
    "\n",
    "Day 13: DBSCAN (Density-Based Spatial Clustering of Applications with Noise)\n",
    "- Concept: Density-based clustering.\n",
    "- Implementation: Epsilon, min samples.\n",
    "- Evaluation: Core points, noise points.\n",
    "\n",
    "Day 14: Linear Discriminant Analysis (LDA)\n",
    "- Concept: Linear combination for classification.\n",
    "- Implementation: Fisher's criterion.\n",
    "- Evaluation: Class separability.\n",
    "\n",
    "Day 15: XGBoost\n",
    "- Concept: Extreme Gradient Boosting.\n",
    "- Implementation: Tree boosting.\n",
    "- Evaluation: Regularization, parallel processing.\n",
    "\n",
    "Day 16: LightGBM\n",
    "- Concept: Gradient boosting framework.\n",
    "- Implementation: Leaf-wise growth.\n",
    "- Evaluation: Speed, accuracy.\n",
    "\n",
    "Day 17: CatBoost\n",
    "- Concept: Gradient boosting with categorical features.\n",
    "- Implementation: Ordered boosting.\n",
    "- Evaluation: Handling of categorical data.\n",
    "\n",
    "Day 18: Neural Networks\n",
    "- Concept: Layers of neurons for learning.\n",
    "- Implementation: Backpropagation.\n",
    "- Evaluation: Activation functions, epochs.\n",
    "\n",
    "Day 19: Convolutional Neural Networks (CNNs)\n",
    "- Concept: Image processing.\n",
    "- Implementation: Convolutions, pooling.\n",
    "- Evaluation: Feature maps, filters.\n",
    "\n",
    "Day 20: Recurrent Neural Networks (RNNs)\n",
    "- Concept: Sequential data processing.\n",
    "- Implementation: Hidden states.\n",
    "- Evaluation: Long-term dependencies.\n",
    "\n",
    "Day 21: Long Short-Term Memory (LSTM)\n",
    "- Concept: Improved RNN.\n",
    "- Implementation: Memory cells.\n",
    "- Evaluation: Forget gates, output gates.\n",
    "\n",
    "Day 22: Gated Recurrent Units (GRU)\n",
    "- Concept: Simplified LSTM.\n",
    "- Implementation: Update gate.\n",
    "- Evaluation: Performance, complexity.\n",
    "\n",
    "Day 23: Autoencoders\n",
    "- Concept: Data compression.\n",
    "- Implementation: Encoder, decoder.\n",
    "- Evaluation: Reconstruction error.\n",
    "\n",
    "Day 24: Generative Adversarial Networks (GANs)\n",
    "- Concept: Generative models.\n",
    "- Implementation: Generator, discriminator.\n",
    "- Evaluation: Adversarial loss.\n",
    "\n",
    "Day 25: Transfer Learning\n",
    "- Concept: Pre-trained models.\n",
    "- Implementation: Fine-tuning.\n",
    "- Evaluation: Domain adaptation.\n",
    "Day 26: Reinforcement Learning\n",
    "- Concept: Learning through interaction.\n",
    "- Implementation: Q-learning.\n",
    "- Evaluation: Reward function, policy.\n",
    "\n",
    "Day 27: Bayesian Networks\n",
    "- Concept: Probabilistic graphical models.\n",
    "- Implementation: Conditional dependencies.\n",
    "- Evaluation: Inference, learning.\n",
    "\n",
    "Day 28: Hidden Markov Models (HMM)\n",
    "- Concept: Time series analysis.\n",
    "- Implementation: Transition probabilities.\n",
    "- Evaluation: Viterbi algorithm.\n",
    "\n",
    "Day 29: Feature Selection Techniques\n",
    "- Concept: Improving model performance.\n",
    "- Implementation: Filter, wrapper methods.\n",
    "- Evaluation: Feature importance.\n",
    "\n",
    "Day 30: Hyperparameter Optimization\n",
    "- Concept: Model tuning.\n",
    "- Implementation: Grid search, random search.\n",
    "- Evaluation: Cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cff25e-778f-4809-87d4-31039909503a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Day 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38df9951-7e39-4268-88e1-4d8aed4566e2",
   "metadata": {},
   "source": [
    "#### Concept\n",
    "Linear regression is a statistical method used to model the relationship between a dependent variable (target) and one or more independent variables (features). The goal is to find the linear equation that best predicts the target variable from the feature variables.\n",
    "\n",
    "The equation of a simple linear regression model is:  \n",
    "\n",
    "$$ y = \\beta_0 + \\beta_1 x $$\n",
    "\n",
    "Where:  \n",
    "\n",
    "- $y$ is the predicted value.  \n",
    "- $\\beta_0$ is the y-intercept.  \n",
    "- $\\beta_1$ is the slope of the line (coefficient).  \n",
    "- $x$ is the independent variable.  \n",
    " \n",
    "\n",
    "\n",
    "#### Implementation\n",
    "\n",
    "Let's consider an example using Python and its libraries.\n",
    "\n",
    "##### Example\n",
    "Suppose we have a dataset with house prices and their corresponding size (in square feet).\n",
    "```python\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example data\n",
    "data = {\n",
    "    'Size': [1500, 1600, 1700, 1800, 1900, 2000, 2100, 2200, 2300, 2400],\n",
    "    'Price': [300000, 320000, 340000, 360000, 380000, 400000, 420000, 440000, 460000, 480000]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Independent variable (feature) and dependent variable (target)\n",
    "X = df[['Size']]\n",
    "y = df['Price']\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Creating and training the linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared: {r2}\")\n",
    "\n",
    "# Plotting the results\n",
    "plt.scatter(X, y, color='blue')  # Original data points\n",
    "plt.plot(X_test, y_pred, color='red', linewidth=2)  # Regression line\n",
    "plt.xlabel('Size (sq ft)')\n",
    "plt.ylabel('Price ($)')\n",
    "plt.title('Linear Regression: House Prices vs Size')\n",
    "plt.show()\n",
    "#### Explanation of the Code\n",
    "```\n",
    "1. Libraries: We import necessary libraries like numpy, pandas, sklearn, and matplotlib.\n",
    "2. Data Preparation: We create a DataFrame containing the size and price of houses.\n",
    "3. Feature and Target: We separate the feature (Size) and the target (Price).\n",
    "4. Train-Test Split: We split the data into training and testing sets.\n",
    "5. Model Training: We create a LinearRegression model and train it using the training data.\n",
    "6. Predictions: We use the trained model to predict house prices for the test set.\n",
    "7. Evaluation: We evaluate the model using Mean Squared Error (MSE) and R-squared (RÂ²) metrics.\n",
    "8. Visualization: We plot the original data points and the regression line to visualize the model's performance.\n",
    "\n",
    "#### Evaluation Metrics\n",
    "\n",
    "- Mean Squared Error (MSE): Measures the average squared difference between the actual and predicted values. Lower values indicate better performance.\n",
    "- R-squared (RÂ²): Represents the proportion of the variance in the dependent variable that is predictable from the independent variable(s). Values closer to 1 indicate a better fit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987caf59-1098-4e3c-824d-c47e0afbef0c",
   "metadata": {},
   "source": [
    "For those of you who are new to Data Science and Machine learning algorithms, let me try to give you a brief overview. ML Algorithms can be categorized into three types: supervised learning, unsupervised learning, and reinforcement learning.\n",
    "\n",
    "1. Supervised Learning:\n",
    "    - Definition: Algorithms learn from labeled training data, making predictions or decisions based on input-output pairs.\n",
    "    - Examples: Linear regression, decision trees, support vector machines (SVM), and neural networks.\n",
    "    - Applications: Email spam detection, image recognition, and medical diagnosis.\n",
    "\n",
    "2. Unsupervised Learning:\n",
    "    - Definition: Algorithms analyze and group unlabeled data, identifying patterns and structures without prior knowledge of the outcomes.\n",
    "    - Examples: K-means clustering, hierarchical clustering, and principal component analysis (PCA).\n",
    "    - Applications: Customer segmentation, market basket analysis, and anomaly detection.\n",
    "\n",
    "3. Reinforcement Learning:\n",
    "    - Definition: Algorithms learn by interacting with an environment, receiving rewards or penalties based on their actions, and optimizing for long-term goals.\n",
    "    - Examples: Q-learning, deep Q-networks (DQN), and policy gradient methods.\n",
    "    - Applications: Robotics, game playing (like AlphaGo), and self-driving cars."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebbcd73-0de6-4909-9818-ece684316c72",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Day 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16505659-7fba-44d6-8fed-8700b2a44b74",
   "metadata": {},
   "source": [
    "Let's start with Day 2 today \n",
    "\n",
    "Let's learn Logistic Regression in detail \n",
    "\n",
    "## Concept\n",
    "Logistic regression is used for binary classification problems, where the outcome is a categorical variable with two possible outcomes (e.g., 0 or 1, true or false). Instead of predicting a continuous value like linear regression, logistic regression predicts the probability of a specific class.\n",
    "\n",
    "The logistic regression model uses the logistic function (also known as the sigmoid function) to map predicted values to probabilities. \n",
    "\n",
    "## Implementation\n",
    "\n",
    "Let's consider an example using Python and its libraries.\n",
    "\n",
    "## Example\n",
    "Suppose we have a dataset that records whether a student has passed an exam based on the number of hours they studied.\n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example data\n",
    "data = {\n",
    "    'Hours_Studied': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'Passed': [0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Independent variable (feature) and dependent variable (target)\n",
    "X = df[['Hours_Studied']]\n",
    "y = df['Passed']\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Creating and training the logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluating the model\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "print(f\"Classification Report:\\n{class_report}\")\n",
    "print(f\"ROC-AUC: {roc_auc}\")\n",
    "\n",
    "# Plotting the ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "## Explanation of the Code\n",
    "\n",
    "1. Libraries: We import necessary libraries like numpy, pandas, sklearn, and matplotlib.\n",
    "2. Data Preparation: We create a DataFrame containing the hours studied and whether the student passed.\n",
    "3. Feature and Target: We separate the feature (Hours_Studied) and the target (Passed).\n",
    "4. Train-Test Split: We split the data into training and testing sets.\n",
    "5. Model Training: We create a LogisticRegression model and train it using the training data.\n",
    "6. Predictions: We use the trained model to predict the pass/fail outcome for the test set and also obtain the predicted probabilities.\n",
    "7. Evaluation: We evaluate the model using the confusion matrix, classification report, and ROC-AUC score.\n",
    "8. Visualization: We plot the ROC curve to visualize the model's performance.\n",
    "\n",
    "## Evaluation Metrics\n",
    "\n",
    "- Confusion Matrix: Shows the counts of true positives, true negatives, false positives, and false negatives.\n",
    "- Classification Report: Provides precision, recall, F1-score, and support for each class.\n",
    "- ROC-AUC: Measures the model's ability to distinguish between the classes. AUC (Area Under the Curve) closer to 1 indicates better performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b93af3-12f0-4552-a5c4-72af1cca8d38",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Day 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4b84db-35f1-4913-a39e-29ea0a5830ad",
   "metadata": {},
   "source": [
    "Let's start with Day 3 today \n",
    "\n",
    "Let's learn Decision Tree in detail \n",
    "\n",
    "#### Concept\n",
    "Decision trees are a non-parametric supervised learning method used for both classification and regression tasks. They model decisions and their possible consequences in a tree-like structure, where internal nodes represent tests on features, branches represent the outcome of the test, and leaf nodes represent the final prediction (class label or value).\n",
    "\n",
    "For classification, decision trees use measures like Gini impurity or entropy to split the data:\n",
    "- Gini Impurity: Measures the likelihood of an incorrect classification of a randomly chosen element.\n",
    "- Entropy (Information Gain): Measures the amount of uncertainty or impurity in the data.\n",
    "\n",
    "For regression, decision trees minimize the variance (mean squared error) in the splits.\n",
    "\n",
    "## Implementation Example\n",
    "Suppose we have a dataset with features like age, income, and student status to predict whether a person buys a computer.\n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example data\n",
    "data = {\n",
    "    'Age': [25, 45, 35, 50, 23, 37, 32, 28, 40, 27],\n",
    "    'Income': ['High', 'High', 'High', 'Medium', 'Low', 'Low', 'Low', 'Medium', 'Low', 'Medium'],\n",
    "    'Student': ['No', 'No', 'No', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No'],\n",
    "    'Buys_Computer': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes']\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Convert categorical features to numeric\n",
    "df['Income'] = df['Income'].map({'Low': 1, 'Medium': 2, 'High': 3})\n",
    "df['Student'] = df['Student'].map({'No': 0, 'Yes': 1})\n",
    "df['Buys_Computer'] = df['Buys_Computer'].map({'No': 0, 'Yes': 1})\n",
    "\n",
    "# Independent variables (features) and dependent variable (target)\n",
    "X = df[['Age', 'Income', 'Student']]\n",
    "y = df['Buys_Computer']\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Creating and training the decision tree model\n",
    "model = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=0)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "print(f\"Classification Report:\\n{class_report}\")\n",
    "\n",
    "# Plotting the decision tree\n",
    "plt.figure(figsize=(12,8))\n",
    "plot_tree(model, feature_names=['Age', 'Income', 'Student'], class_names=['No', 'Yes'], filled=True)\n",
    "plt.title('Decision Tree')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "#### Explanation of the Code\n",
    "\n",
    "1. Libraries: We import necessary libraries like numpy, pandas, sklearn, and matplotlib.\n",
    "2. Data Preparation: We create a DataFrame containing features and the target variable. Categorical features are converted to numeric values.\n",
    "3. Feature and Target: We separate the features (Age, Income, Student) and the target (Buys_Computer).\n",
    "4. Train-Test Split: We split the data into training and testing sets.\n",
    "5. Model Training: We create a DecisionTreeClassifier model, specifying the criterion (Gini impurity) and maximum depth of the tree, and train it using the training data.\n",
    "6. Predictions: We use the trained model to predict whether a person buys a computer for the test set.\n",
    "7. Evaluation: Evaluate the model using accuracy, confusion matrix, and classification report.\n",
    "8. Visualization: Plot decision tree to visualize the decision-making process.\n",
    "\n",
    "## Evaluation Metrics\n",
    "\n",
    "- Accuracy\n",
    "\n",
    "- Confusion Matrix: Shows the counts of true positives, true negatives, false positives, and false negatives.\n",
    "\n",
    "- Classification Report: Provides precision, recall, F1-score, and support for each class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aceeb938-0067-4aca-bd04-ee18e16250a4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Day 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7c8481-65a2-4194-9a07-ab665d11ff78",
   "metadata": {},
   "source": [
    "Let's start with Day 4 today \n",
    "\n",
    "Let's learn Random Forest in detail \n",
    "\n",
    "#### Concept\n",
    "Random Forest is an ensemble learning method that combines multiple decision trees to improve classification or regression performance. Each tree in the forest is built on a random subset of the data and a random subset of features. The final prediction is made by aggregating the predictions from all individual trees (majority vote for classification, average for regression).\n",
    "\n",
    "Key advantages of Random Forest include:\n",
    "- Reduced Overfitting: By averaging multiple trees, Random Forest reduces the risk of overfitting compared to individual decision trees.\n",
    "- Robustness: Less sensitive to the variability in the data.\n",
    "\n",
    "## Implementation Example\n",
    "Suppose we have a dataset that records whether a patient has a heart disease based on features like age, cholesterol level, and maximum heart rate.\n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Example data\n",
    "data = {\n",
    "    'Age': [29, 45, 50, 39, 48, 50, 55, 60, 62, 43],\n",
    "    'Cholesterol': [220, 250, 230, 180, 240, 290, 310, 275, 300, 280],\n",
    "    'Max_Heart_Rate': [180, 165, 170, 190, 155, 160, 150, 140, 130, 148],\n",
    "    'Heart_Disease': [0, 1, 1, 0, 1, 1, 1, 1, 1, 0]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Independent variables (features) and dependent variable (target)\n",
    "X = df[['Age', 'Cholesterol', 'Max_Heart_Rate']]\n",
    "y = df['Heart_Disease']\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Creating and training the random forest model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "print(f\"Classification Report:\\n{class_report}\")\n",
    "\n",
    "# Feature importance\n",
    "feature_importances = pd.DataFrame(model.feature_importances_, index=X.columns, columns=['Importance']).sort_values('Importance', ascending=False)\n",
    "print(f\"Feature Importances:\\n{feature_importances}\")\n",
    "\n",
    "# Plotting the feature importances\n",
    "sns.barplot(x=feature_importances.index, y=feature_importances['Importance'])\n",
    "plt.title('Feature Importances')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Importance')\n",
    "plt.show()\n",
    "```\n",
    "## Explanation of the Code\n",
    "\n",
    "1. Libraries: We import necessary libraries like numpy, pandas, sklearn, matplotlib, and seaborn.\n",
    "2. Data Preparation: We create a DataFrame containing features (Age, Cholesterol, Max_Heart_Rate) and the target variable (Heart_Disease).\n",
    "3. Feature and Target: We separate the features and the target variable.\n",
    "4. Train-Test Split: We split the data into training and testing sets.\n",
    "5. Model Training: We create a RandomForestClassifier model with 100 trees and train it using the training data.\n",
    "6. Predictions: We use the trained model to predict heart disease for the test set.\n",
    "7. Evaluation: We evaluate the model using accuracy, confusion matrix, and classification report.\n",
    "8. Feature Importance: We compute and display the importance of each feature.\n",
    "9. Visualization: We plot the feature importances to visualize which features contribute most to the model's predictions.\n",
    "\n",
    "## Evaluation Metrics\n",
    "\n",
    "- Accuracy: The proportion of correctly classified instances among the total instances.\n",
    "- Confusion Matrix: Shows the counts of true positives, true negatives, false positives, and false negatives.\n",
    "- Classification Report: Provides precision, recall, F1-score, and support for each class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35fd9ee-1a0a-495e-8c5b-5aee267bcbc5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Day 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae165279-8a76-443d-8df7-ff926a6fadc0",
   "metadata": {},
   "source": [
    "Let's start with Day 5 today \n",
    "\n",
    "Let's learn Gradient Boosting in detail \n",
    "\n",
    "Concept: Gradient Boosting is an ensemble learning technique that builds a strong predictive model by combining the predictions of multiple weaker models, typically decision trees. Unlike Random Forest, which builds trees independently, Gradient Boosting builds trees sequentially, each one correcting the errors of its predecessor.\n",
    "\n",
    "The key idea is to optimize a loss function over the iterations:\n",
    "1. Initialize the model with a constant value.\n",
    "2. Fit a weak learner (e.g., a decision tree) to the residuals (errors) of the previous model.\n",
    "3. Update the model by adding the fitted weak learner to minimize the loss.\n",
    "4. Repeat the process for a specified number of iterations or until convergence.\n",
    "\n",
    "## Implementation Example\n",
    "\n",
    "Suppose we have a dataset that records features like age, income, and years of experience to predict whether a person gets a loan approval.\n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Example data\n",
    "data = {\n",
    "    'Age': [25, 45, 35, 50, 23, 37, 32, 28, 40, 27],\n",
    "    'Income': [50000, 60000, 70000, 80000, 20000, 30000, 40000, 55000, 65000, 75000],\n",
    "    'Years_Experience': [1, 20, 10, 25, 2, 5, 7, 3, 15, 12],\n",
    "    'Loan_Approved': [0, 1, 1, 1, 0, 0, 1, 0, 1, 1]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Independent variables (features) and dependent variable (target)\n",
    "X = df[['Age', 'Income', 'Years_Experience']]\n",
    "y = df['Loan_Approved']\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Creating and training the gradient boosting model\n",
    "model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=0)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "print(f\"Classification Report:\\n{class_report}\")\n",
    "\n",
    "# Feature importance\n",
    "feature_importances = pd.DataFrame(model.feature_importances_, index=X.columns, columns=['Importance']).sort_values('Importance', ascending=False)\n",
    "print(f\"Feature Importances:\\n{feature_importances}\")\n",
    "\n",
    "# Plotting the feature importances\n",
    "sns.barplot(x=feature_importances.index, y=feature_importances['Importance'])\n",
    "plt.title('Feature Importances')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Importance')\n",
    "plt.show()\n",
    "```\n",
    "## Explanation of the Code\n",
    "\n",
    "1. Libraries: We import necessary libraries like numpy, pandas, sklearn, matplotlib, and seaborn.\n",
    "2. Data Preparation: We create a DataFrame containing features (Age, Income, Years_Experience) and the target variable (Loan_Approved).\n",
    "3. Feature and Target: We separate the features and the target variable.\n",
    "4. Train-Test Split: We split the data into training and testing sets.\n",
    "5. Model Training: We create a GradientBoostingClassifier model with 100 estimators (n_estimators=100), a learning rate of 0.1, and a maximum depth of 3, and train it using the training data.\n",
    "6. Predictions: We use the trained model to predict loan approval for the test set.\n",
    "7. Evaluation: We evaluate the model using accuracy, confusion matrix, and classification report.\n",
    "8. Feature Importance: We compute and display the importance of each feature.\n",
    "9. Visualization: We plot the feature importances to visualize which features contribute most to the model's predictions.\n",
    "\n",
    "## Evaluation Metrics\n",
    "\n",
    "- Accuracy: The proportion of correctly classified instances among the total instances.\n",
    "- Confusion Matrix: Counts of TP, TN, FP, and FN.\n",
    "- Classification Report: Provides precision, recall, F1-score, and support for each class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49c6bc6-fc5c-4ab5-809d-b9ffa7af5f7f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Day 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee198a5-1373-4d2b-89be-573331730bc1",
   "metadata": {},
   "source": [
    "Let's start with Day 6 today \n",
    "\n",
    "Let's learn Support Vector Machine in detail \n",
    "\n",
    "Concept: Support Vector Machines (SVM) are supervised learning models used for classification and regression tasks. The goal of SVM is to find the optimal hyperplane that maximally separates the classes in the feature space. The hyperplane is chosen to maximize the margin, which is the distance between the hyperplane and the nearest data points from each class, known as support vectors.\n",
    "\n",
    "For nonlinear data, SVM uses a kernel trick to transform the input features into a higher-dimensional space where a linear separation is possible. Common kernels include:\n",
    "- Linear Kernel\n",
    "- Polynomial Kernel\n",
    "- Radial Basis Function (RBF) Kernel\n",
    "- Sigmoid Kernel\n",
    "\n",
    "## Implementation Example\n",
    "Suppose we have a dataset that records features like petal length and petal width to classify the species of iris flowers.\n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Example data (Iris dataset)\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X = iris.data[:, 2:4]Â  # Using petal length and petal width as features\n",
    "y = iris.target\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Creating and training the SVM model with RBF kernel\n",
    "model = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=0)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "print(f\"Classification Report:\\n{class_report}\")\n",
    "\n",
    "# Plotting the decision boundary\n",
    "def plot_decision_boundary(X, y, model):\n",
    "Â Â Â  h = .02Â  # step size in the mesh\n",
    "Â Â Â  x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "Â Â Â  y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "Â Â Â  xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "Â Â Â  Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Â Â Â  Z = Z.reshape(xx.shape)\n",
    "Â Â Â  plt.contourf(xx, yy, Z, alpha=0.8)\n",
    "\n",
    "Â Â Â  sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y, palette='bright', edgecolor='k', s=50)\n",
    "Â Â Â  plt.xlabel('Petal Length')\n",
    "Â Â Â  plt.ylabel('Petal Width')\n",
    "Â Â Â  plt.title('SVM Decision Boundary')\n",
    "Â Â Â  plt.show()\n",
    "\n",
    "plot_decision_boundary(X_test, y_test, model)\n",
    "```\n",
    "\n",
    "#### Explanation of the Code\n",
    "\n",
    "1. Importing Libraries\n",
    "2. Data Preparation\n",
    "3. Train-Test Split\n",
    "4. Model Training: We create an SVC model with an RBF kernel (kernel='rbf'), regularization parameter C=1.0, and gamma parameter set to 'scale', and train it using the training data.\n",
    "5. Predictions: We use the trained model to predict the species of iris flowers for the test set.\n",
    "6. Evaluation: We evaluate the model using accuracy, confusion matrix, and classification report.\n",
    "7. Visualization: Plot the decision boundary to visualize how the SVM separates the classes.\n",
    "\n",
    "#### Decision Boundary\n",
    "\n",
    "The decision boundary plot helps to visualize how the SVM model separates the different classes in the feature space. The SVM with an RBF kernel can capture more complex relationships than a linear classifier.\n",
    "\n",
    "SVMs are powerful for high-dimensional spaces and effective when the number of dimensions is greater than the number of samples. However, they can be memory-intensive and require careful tuning of hyperparameters such as the regularization parameter $C\\$ and kernel parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98930bf-5369-4d80-96fe-b8de3947a572",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Day 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5070c9-6afd-4d27-a534-16f0bc9990a4",
   "metadata": {},
   "source": [
    "Let's start with Day 7 today \n",
    "\n",
    "Let's learn K-Nearest Neighbors (KNN) today \n",
    "\n",
    "Concept: K-Nearest Neighbors (KNN) is a simple, instance-based learning algorithm used for both classification and regression tasks. The main idea is to predict the value or class of a new sample based on the \\( k \\) closest samples (neighbors) in the training dataset.\n",
    "\n",
    "For classification, the predicted class is the most common class among the \\( k \\) nearest neighbors. For regression, the predicted value is the average (or weighted average) of the values of the \\( k \\) nearest neighbors.\n",
    "\n",
    "Key points:\n",
    "- Distance Metric: Common distance metrics include Euclidean distance, Manhattan distance, and Minkowski distance.\n",
    "- Choosing \\( k \\): The value of \\( k \\) is a crucial hyperparameter that needs to be chosen carefully. Smaller \\( k \\) values can lead to noise sensitivity, while larger \\( k \\) values can smooth out the decision boundary.\n",
    "\n",
    "## Implementation Example\n",
    "Suppose we have a dataset that records features like sepal length and sepal width to classify the species of iris flowers.\n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Example data (Iris dataset)\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X = iris.data[:, :2]  # Using sepal length and sepal width as features\n",
    "y = iris.target\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Creating and training the KNN model with k=5\n",
    "model = KNeighborsClassifier(n_neighbors=5)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "print(f\"Classification Report:\\n{class_report}\")\n",
    "\n",
    "# Plotting the decision boundary\n",
    "def plot_decision_boundary(X, y, model):\n",
    "    h = .02  # step size in the mesh\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, alpha=0.8)\n",
    "\n",
    "    sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y, palette='bright', edgecolor='k', s=50)\n",
    "    plt.xlabel('Sepal Length')\n",
    "    plt.ylabel('Sepal Width')\n",
    "    plt.title('KNN Decision Boundary')\n",
    "    plt.show()\n",
    "\n",
    "plot_decision_boundary(X_test, y_test, model)\n",
    "```\n",
    "\n",
    "#### Explanation of the Code\n",
    "\n",
    "1. Libraries\n",
    "2. Data Preparation\n",
    "3. Train-Test Split\n",
    "4. Model Training\n",
    "5. Predictions\n",
    "6. Evaluation.\n",
    "7. Visualization: We plot the decision boundary to visualize how the KNN classifier separates the classes.\n",
    "\n",
    "#### Evaluation Metrics\n",
    "\n",
    "- Confusion Matrix: Shows the counts of true positives, true negatives, false positives, and false negatives.\n",
    "- Classification Report: Provides precision, recall, F1-score, and support for each class.\n",
    "\n",
    "#### Decision Boundary\n",
    "\n",
    "The decision boundary plot helps to visualize how the KNN classifier separates the different classes in the feature space. KNN decision boundaries can be quite complex, reflecting the non-linear separability of the data.\n",
    "\n",
    "KNN is intuitive and simple but can be computationally expensive, especially with large datasets, since it requires storing and searching through all training instances during prediction. The choice of $ k \\$ and the distance metric are critical to the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1a12e5-6d3e-482d-916f-ee99a2648317",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Day 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ca2a32-d42f-4804-a83b-a4c74b175790",
   "metadata": {},
   "source": [
    "Let's start with Day 8 today \n",
    "\n",
    "Let's learn about Naive Bayes Algorithm today\n",
    "\n",
    "Concept: Naive Bayes is a family of probabilistic algorithms based on Bayes' Theorem with the \"naive\" assumption of independence between every pair of features. Despite this strong assumption, Naive Bayes classifiers have performed surprisingly well in many real-world applications, particularly for text classification.\n",
    "\n",
    "#### Types of Naive Bayes Classifiers\n",
    "1. Gaussian Naive Bayes: Assumes that the features follow a normal distribution.\n",
    "2. Multinomial Naive Bayes: Typically used for discrete data (e.g., text classification with word counts).\n",
    "3. Bernoulli Naive Bayes: Used for binary/boolean features.\n",
    "\n",
    "#### Implementation\n",
    "\n",
    "Let's consider an example using Python and its libraries.\n",
    "\n",
    "##### Example\n",
    "Suppose we have a dataset that records features of different emails, such as word frequencies, to classify them as spam or not spam.\n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Example data\n",
    "data = {\n",
    "    'Feature1': [1, 2, 3, 4, 5, 1, 2, 3, 4, 5],\n",
    "    'Feature2': [5, 4, 3, 2, 1, 5, 4, 3, 2, 1],\n",
    "    'Feature3': [1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
    "    'Spam': [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Independent variables (features) and dependent variable (target)\n",
    "X = df[['Feature1', 'Feature2', 'Feature3']]\n",
    "y = df['Spam']\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Creating and training the Multinomial Naive Bayes model\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "print(f\"Classification Report:\\n{class_report}\")\n",
    "```\n",
    "\n",
    "#### Explanation of the Code\n",
    "\n",
    "1. Libraries: We import necessary libraries like numpy, pandas, and sklearn.\n",
    "2. Data Preparation: We create a DataFrame containing features (Feature1, Feature2, Feature3) and the target variable (Spam).\n",
    "3. Feature and Target: We separate the features and the target variable.\n",
    "4. Train-Test Split: We split the data into training and testing sets.\n",
    "5. Model Training: We create a MultinomialNB model and train it using the training data.\n",
    "6. Predictions: We use the trained model to predict whether the emails in the test set are spam.\n",
    "7. Evaluation: We evaluate the model using accuracy, confusion matrix, and classification report.\n",
    "\n",
    "#### Evaluation Metrics\n",
    "\n",
    "- Accuracy: The proportion of correctly classified instances among the total instances.\n",
    "- Confusion Matrix: Shows the counts of true positives, true negatives, false positives, and false negatives.\n",
    "- Classification Report: Provides precision, recall, F1-score, and support for each class.\n",
    "\n",
    "#### Applications\n",
    "\n",
    "Naive Bayes classifiers are widely used for:\n",
    "- Text Classification: Spam detection, sentiment analysis, and document categorization.\n",
    "- Medical Diagnosis: Predicting diseases based on symptoms.\n",
    "- Recommendation Systems: Recommending products or services based on user behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f52b02f-2690-4803-a02b-a23b0862e835",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Day 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18680f3e-a180-4822-a196-1852357c37cc",
   "metadata": {},
   "source": [
    "Let's start with Day 9 today \n",
    "\n",
    "Let's learn about Principal Component Analysis (PCA) today \n",
    "\n",
    "Concept: Principal Component Analysis (PCA) is a dimensionality reduction technique used to transform a large set of correlated features into a smaller set of uncorrelated features called principal components. These principal components capture the maximum variance in the data while reducing the dimensionality.\n",
    "\n",
    "The steps involved in PCA are:\n",
    "1. Standardization: Normalize the data to have zero mean and unit variance.\n",
    "2. Covariance Matrix Computation: Compute the covariance matrix of the features.\n",
    "3. Eigenvalue and Eigenvector Decomposition: Compute the eigenvalues and eigenvectors of the covariance matrix.\n",
    "4. Principal Components Selection: Select the top \\(k\\) eigenvectors corresponding to the largest eigenvalues to form the principal components.\n",
    "5. Transformation: Project the original data onto the new subspace formed by the selected principal components.\n",
    "\n",
    "#### Benefits of PCA\n",
    "- Reduces Dimensionality: Simplifies the dataset by reducing the number of features.\n",
    "- Improves Performance: Speeds up machine learning algorithms and reduces the risk of overfitting.\n",
    "- Uncovers Hidden Patterns: Helps visualize the underlying structure of the data.\n",
    "\n",
    "#### Implementation\n",
    "\n",
    "Let's consider an example using Python and its libraries.\n",
    "\n",
    "##### Example\n",
    "Suppose we have a dataset with multiple features and we want to reduce the dimensionality using PCA.\n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example data (Iris dataset)\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Standardizing the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Applying PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Plotting the principal components\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', edgecolor='k', s=50)\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('PCA of Iris Dataset')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "# Explained variance\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "print(f\"Explained Variance by Component 1: {explained_variance[0]:.2f}\")\n",
    "print(f\"Explained Variance by Component 2: {explained_variance[1]:.2f}\")\n",
    "```\n",
    "\n",
    "#### Explanation of the Code\n",
    "\n",
    "1. Libraries: We import necessary libraries like numpy, pandas, sklearn, and matplotlib.\n",
    "2. Data Preparation: We use the Iris dataset with four features.\n",
    "3. Standardization: We standardize the features to have zero mean and unit variance.\n",
    "4. Applying PCA: We create a PCA object with 2 components and fit it to the standardized data, then transform the data to the new 2-dimensional subspace.\n",
    "5. Plotting: We scatter plot the principal components with color indicating different classes.\n",
    "6. Explained Variance: We print the proportion of variance explained by the first two principal components.\n",
    "\n",
    "#### Explained Variance\n",
    "\n",
    "- Explained Variance: Indicates how much of the total variance in the data is captured by each principal component. In our example, if the first principal component explains 72% of the variance and the second explains 23%, together they explain 95% of the variance.\n",
    "\n",
    "#### Applications\n",
    "\n",
    "PCA is widely used in:\n",
    "- Data Visualization: Reducing high-dimensional data to 2 or 3 dimensions for visualization.\n",
    "- Noise Reduction: Removing noise by retaining only the principal components with significant variance.\n",
    "- Feature Extraction: Deriving new features that capture the essential information.\n",
    "\n",
    "PCA is a powerful tool for simplifying complex datasets while retaining the most important information. However, it assumes linear relationships among variables and may not capture complex patterns in the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dc17e6-a776-4bf4-99d4-5d3d0a36e27a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Day 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918b0d8d-1ca2-4682-950b-3b691b4e8b6d",
   "metadata": {},
   "source": [
    "Let's start with Day 10 today \n",
    "\n",
    "Let's learn about k-Means Clustering today \n",
    "\n",
    "Concept: k-Means is an unsupervised learning algorithm used for clustering tasks. The goal is to partition a dataset into $ k $ clusters, where each data point belongs to the cluster with the nearest mean. It is an iterative algorithm that aims to minimize the variance within each cluster.\n",
    "\n",
    "The steps involved in k-Means clustering are:\n",
    "1. Initialization: Choose $ k \\$ initial cluster centroids randomly.\n",
    "2. Assignment: Assign each data point to the nearest cluster centroid.\n",
    "3. Update: Recalculate the centroids as the mean of all points in each cluster.\n",
    "4. Repeat: Repeat steps 2 and 3 until the centroids do not change significantly or a maximum number of iterations is reached.\n",
    "\n",
    "#### Implementation Example\n",
    "Suppose we have a dataset with points in 2D space, and we want to cluster them into $ k = 3 $ clusters.\n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Example data\n",
    "np.random.seed(0)\n",
    "X = np.vstack((np.random.normal(0, 1, (100, 2)),\n",
    "               np.random.normal(5, 1, (100, 2)),\n",
    "               np.random.normal(-5, 1, (100, 2))))\n",
    "\n",
    "# Applying k-Means clustering\n",
    "k = 3\n",
    "kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "y_kmeans = kmeans.fit_predict(X)\n",
    "\n",
    "# Plotting the clusters\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y_kmeans, palette='viridis', s=50, edgecolor='k')\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='red', label='Centroids')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('k-Means Clustering')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "## Explanation of the Code\n",
    "\n",
    "1. Libraries: We import necessary libraries like numpy, pandas, sklearn, matplotlib, and seaborn.\n",
    "2. Data Preparation: We generate a synthetic dataset with three clusters using normal distributions.\n",
    "3. k-Means Clustering: We create a KMeans object with $ k=3 $ clusters and fit it to the data. The fit_predict method assigns each data point to a cluster.\n",
    "4. Plotting: We scatter plot the data points with colors indicating the assigned clusters and plot the centroids in red.\n",
    "\n",
    "#### Choosing the Number of Clusters\n",
    "\n",
    "Selecting the appropriate number of clusters $ k $ is crucial. Common methods to determine $ k $ include:\n",
    "- Elbow Method: Plot the within-cluster sum of squares (WCSS) against the number of clusters and look for an \"elbow\" point where the rate of decrease sharply slows.\n",
    "- Silhouette Score: Measures how similar an object is to its own cluster compared to other clusters. Higher silhouette scores indicate better-defined clusters.\n",
    "\n",
    "## Elbow Method Example\n",
    "\n",
    "```python\n",
    "# Elbow Method to find the optimal number of clusters\n",
    "wcss = []\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=i, random_state=0)\n",
    "    kmeans.fit(X)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(range(1, 11), wcss, marker='o')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.title('Elbow Method')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "## Evaluation Metrics\n",
    "\n",
    "- Within-Cluster Sum of Squares (WCSS): Measures the compactness of the clusters. Lower WCSS indicates more compact clusters.\n",
    "- Silhouette Score: Measures the separation between clusters. Values range from -1 to 1, with higher values indicating better-defined clusters.\n",
    "\n",
    "#### Applications\n",
    "\n",
    "k-Means clustering is widely used in:\n",
    "- Market Segmentation: Grouping customers based on purchasing behavior.\n",
    "- Image Compression: Reducing the number of colors in an image.\n",
    "- Anomaly Detection: Identifying outliers in a dataset.\n",
    "\n",
    "k-Means is efficient and easy to implement but can be sensitive to the initial placement of centroids and the choice of $ k $. It works well for spherical clusters but may struggle with non-spherical or overlapping clusters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188b820c-a559-4cf7-90ac-f0168eee99a4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Day 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53450879-bace-4418-bd8a-82374b6b8dd4",
   "metadata": {},
   "source": [
    "Let's start with Day 11 today \n",
    "\n",
    "Let's learn about Hierarchical Clustering\n",
    "\n",
    "## Concept: Hierarchical clustering is an unsupervised learning algorithm used to build a hierarchy of clusters. It seeks to create a tree of clusters called a dendrogram, which can then be used to decide the level at which to cut the tree to form clusters. There are two main types of hierarchical clustering:\n",
    "\n",
    "1. Agglomerative Hierarchical Clustering (Bottom-Up):\n",
    "    - Starts with each data point as a single cluster.\n",
    "    - Iteratively merges the closest pairs of clusters until all points are in a single cluster or the desired number of clusters is reached.\n",
    "\n",
    "2. Divisive Hierarchical Clustering (Top-Down):\n",
    "    - Starts with all data points in a single cluster.\n",
    "    - Iteratively splits the most heterogeneous cluster until each data point is in its own cluster or the desired number of clusters is reached.\n",
    "\n",
    "## Linkage Criteria\n",
    "The choice of how to measure the distance between clusters affects the structure of the dendrogram:\n",
    "- Single Linkage: Minimum distance between points in two clusters.\n",
    "- Complete Linkage: Maximum distance between points in two clusters.\n",
    "- Average Linkage: Average distance between points in two clusters.\n",
    "- Ward's Method: Minimizes the variance within clusters.\n",
    "\n",
    "## Implementation Example\n",
    "\n",
    "Suppose we have a dataset with points in 2D space, and we want to cluster them using hierarchical clustering.\n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Example data\n",
    "np.random.seed(0)\n",
    "X = np.vstack((np.random.normal(0, 1, (100, 2)),\n",
    "               np.random.normal(5, 1, (100, 2)),\n",
    "               np.random.normal(-5, 1, (100, 2))))\n",
    "\n",
    "# Performing hierarchical clustering\n",
    "Z = linkage(X, method='ward')\n",
    "\n",
    "# Plotting the dendrogram\n",
    "plt.figure(figsize=(10, 7))\n",
    "dendrogram(Z, truncate_mode='level', p=5, leaf_rotation=90., leaf_font_size=12., show_contracted=True)\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('Sample index')\n",
    "plt.ylabel('Distance')\n",
    "plt.show()\n",
    "\n",
    "# Cutting the dendrogram to form clusters\n",
    "max_d = 7.0  # Example threshold for cutting the dendrogram\n",
    "clusters = fcluster(Z, max_d, criterion='distance')\n",
    "\n",
    "# Plotting the clusters\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=clusters, palette='viridis', s=50, edgecolor='k')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Hierarchical Clustering')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "## Explanation of the Code\n",
    "\n",
    "1. Importing Libraries\n",
    "2. Data Preparation: We generate a synthetic dataset with three clusters using normal distributions.\n",
    "3. Linkage: We use the linkage function from scipy.cluster.hierarchy to perform hierarchical clustering with Ward's method.\n",
    "4. Dendrogram: We plot the dendrogram using the dendrogram function to visualize the hierarchical structure.\n",
    "5. Cutting the Dendrogram: We cut the dendrogram at a specific threshold to form clusters using the fcluster function.\n",
    "6. Plotting Clusters: We scatter plot the data points with colors indicating the assigned clusters.\n",
    "\n",
    "#### Choosing the Number of Clusters\n",
    "\n",
    "The dendrogram helps visualize the hierarchy of clusters. The choice of where to cut the dendrogram (i.e., selecting a threshold distance) determines the number of clusters. This choice can be subjective, but some guidelines include:\n",
    "- Elbow Method: Similar to k-Means, look for an \"elbow\" in the dendrogram where the distance between merges increases significantly.\n",
    "- Maximum Distance: Choose a distance threshold that balances the number of clusters and the compactness of clusters.\n",
    "\n",
    "## Applications\n",
    "\n",
    "Hierarchical clustering is widely used in:\n",
    "- Gene Expression Data: Grouping similar genes or samples in bioinformatics.\n",
    "- Document Clustering: Organizing documents into a hierarchical structure.\n",
    "- Image Segmentation: Dividing an image into regions based on pixel similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8ba492-67c2-46de-ba80-0212b5b258e1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Day 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7611b8e4-0558-4ac1-93df-c06d536ede6a",
   "metadata": {},
   "source": [
    "Let's start with Day 12 today \n",
    "\n",
    "Let's learn about Association Rule Learning\n",
    "\n",
    "Concept: Association rule learning is a rule-based machine learning method used to discover interesting relations between variables in large databases. It is widely used in market basket analysis to identify sets of products that frequently co-occur in transactions. The main goal is to find strong rules discovered in databases using some measures of interestingness.\n",
    "\n",
    "#### Key Terms\n",
    "- Support: The proportion of transactions in the dataset that contain a particular itemset.\n",
    "- Confidence: The likelihood that a transaction containing an itemset A also contains an itemset B . \n",
    "- Lift: The ratio of the observed support to that expected if A and B  were independent. \n",
    "\n",
    "#### Algorithm\n",
    "The most common algorithm for association rule learning is the Apriori algorithm. It operates in two steps:\n",
    "1. Frequent Itemset Generation: Identify all itemsets whose support is greater than or equal to a specified minimum support threshold.\n",
    "2. Rule Generation: From the frequent itemsets, generate high-confidence rules where confidence is greater than or equal to a specified minimum confidence threshold.\n",
    "\n",
    "#### Implementation\n",
    "\n",
    "Let's consider an example using Python and its libraries.\n",
    "\n",
    "##### Example\n",
    "Suppose we have a dataset of transactions, and we want to identify frequent itemsets and generate association rules.\n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "# Example data: list of transactions\n",
    "data = {'TransactionID': [1, 1, 1, 2, 2, 3, 3, 3, 4, 4, 4, 4],\n",
    "        'Item': ['Milk', 'Bread', 'Butter', 'Bread', 'Butter', 'Milk', 'Bread', 'Eggs', 'Milk', 'Bread', 'Butter', 'Eggs']}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df = df.groupby(['TransactionID', 'Item'])['Item'].count().unstack().reset_index().fillna(0).set_index('TransactionID')\n",
    "df = df.applymap(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "# Applying the Apriori algorithm\n",
    "frequent_itemsets = apriori(df, min_support=0.5, use_colnames=True)\n",
    "\n",
    "# Generating association rules\n",
    "rules = association_rules(frequent_itemsets, metric='confidence', min_threshold=0.7)\n",
    "\n",
    "print(\"Frequent Itemsets:\")\n",
    "print(frequent_itemsets)\n",
    "print(\"\\nAssociation Rules:\")\n",
    "print(rules)\n",
    "```\n",
    "\n",
    "#### Explanation of the Code\n",
    "\n",
    "1. Libraries: We import necessary libraries like pandas and mlxtend.\n",
    "2. Data Preparation: We create a transaction dataset and transform it into a format suitable for the Apriori algorithm, where each row represents a transaction and each column represents an item.\n",
    "3. Apriori Algorithm: We apply the Apriori algorithm to find frequent itemsets with a minimum support of 0.5.\n",
    "4. Association Rules: We generate association rules from the frequent itemsets with a minimum confidence of 0.7.\n",
    "\n",
    "#### Evaluation Metrics\n",
    "\n",
    "- Support: Measures the frequency of an itemset in the dataset.\n",
    "- Confidence: Measures the reliability of the inference made by the rule.\n",
    "- Lift: Measures the strength of the rule over random co-occurrence. Lift values greater than 1 indicate a strong association.\n",
    "\n",
    "#### Applications\n",
    "\n",
    "Association rule learning is widely used in:\n",
    "- Market Basket Analysis: Identifying products frequently bought together to optimize store layouts and cross-selling strategies.\n",
    "- Recommendation Systems: Recommending products or services based on customer purchase history.\n",
    "- Healthcare: Discovering associations between medical conditions and treatments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35e7a14-685f-41e9-a92e-40f60c7bc3af",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Day 13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b499ea-0891-405e-a36a-831a2e643b2e",
   "metadata": {},
   "source": [
    "Let's start with Day 13 today \n",
    "\n",
    "Let's learn about DBSCAN (Density-Based Spatial Clustering of Applications with Noise)\n",
    "\n",
    "#### Concept\n",
    "DBSCAN is an unsupervised clustering algorithm that groups together points that are closely packed, and marks points that are in low-density regions as outliers. It is particularly effective for identifying clusters of arbitrary shape and handling noise in the data.\n",
    "\n",
    "#### Key Parameters\n",
    "- Epsilon (Îµ): The maximum distance between two points to be considered neighbors.\n",
    "- MinPts: The minimum number of points required to form a dense region (a cluster).\n",
    "\n",
    "#### Key Terms\n",
    "- Core Point: A point with at least MinPts neighbors within a radius of Îµ.\n",
    "- Border Point: A point that is not a core point but is within the neighborhood of a core point.\n",
    "- Noise Point: A point that is neither a core point nor a border point (outlier).\n",
    "\n",
    "#### Algorithm Steps\n",
    "1. Identify Core Points: For each point in the dataset, find its Îµ-neighborhood. If it contains at least MinPts points, mark it as a core point.\n",
    "2. Expand Clusters: From each core point, recursively collect directly density-reachable points to form a cluster.\n",
    "3. Label Border and Noise Points: Points that are reachable from core points but not core points themselves are labeled as border points. Points that are not reachable from any core point are labeled as noise.\n",
    "\n",
    "#### Implementation\n",
    "\n",
    "Let's consider an example using Python and its libraries.\n",
    "\n",
    "##### Example\n",
    "Suppose we have a dataset with points in a 2D space, and we want to cluster them using DBSCAN.\n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.cluster import DBSCAN\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Generate example data (make_moons dataset)\n",
    "X, y = make_moons(n_samples=300, noise=0.1, random_state=0)\n",
    "\n",
    "# Applying DBSCAN\n",
    "epsilon = 0.2\n",
    "min_samples = 5\n",
    "db = DBSCAN(eps=epsilon, min_samples=min_samples)\n",
    "clusters = db.fit_predict(X)\n",
    "\n",
    "# Adding cluster labels to the dataframe\n",
    "df = pd.DataFrame(X, columns=['Feature 1', 'Feature 2'])\n",
    "df['Cluster'] = clusters\n",
    "\n",
    "# Plotting the clusters\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x='Feature 1', y='Feature 2', hue='Cluster', palette='Set1', data=df)\n",
    "plt.title('DBSCAN Clustering')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "#### Explanation of the Code\n",
    "\n",
    "1. Libraries: We import necessary libraries like numpy, pandas, sklearn, matplotlib, and seaborn.\n",
    "2. Data Preparation: We generate a synthetic dataset using make_moons with two features.\n",
    "3. Applying DBSCAN: We apply the DBSCAN algorithm with specified epsilon and min_samples values to cluster the data.\n",
    "4. Adding Cluster Labels: We create a DataFrame with the features and cluster labels.\n",
    "5. Plotting: We scatter plot the data points with colors indicating different clusters.\n",
    "\n",
    "#### Choosing Parameters\n",
    "\n",
    "Choosing appropriate values for Îµ and MinPts is crucial:\n",
    "- Epsilon (Îµ): Often determined using a k-distance graph where k = MinPts - 1. A sudden change in the slope can suggest a good value for Îµ.\n",
    "- MinPts: Typically set to at least the dimensionality of the dataset plus one. For 2D data, a common value is 4 or 5.\n",
    "\n",
    "#### Handling Outliers\n",
    "\n",
    "DBSCAN can identify outliers as noise points. These are points that do not belong to any cluster, making DBSCAN robust to noise in the data.\n",
    "\n",
    "#### Applications\n",
    "\n",
    "DBSCAN is widely used in:\n",
    "- Geospatial Data Analysis: Identifying regions of interest in spatial data.\n",
    "- Image Segmentation: Grouping pixels into regions based on their intensity.\n",
    "- Anomaly Detection: Identifying unusual patterns or outliers in datasets.\n",
    "\n",
    "DBSCAN is powerful for discovering clusters of arbitrary shape and handling noise effectively. However, it can struggle with varying densities and requires careful tuning of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a27dcd-6abc-49e3-814c-017cee4fcf70",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Day 14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46287c1-8ec1-4646-b12b-546b2992bd8b",
   "metadata": {},
   "source": [
    "Let's start with Day 14 today\n",
    "\n",
    "Let's learn about Linear Discriminant Analysis (LDA)\n",
    "\n",
    "Concept: Linear Discriminant Analysis (LDA) is a classification and dimensionality reduction technique that aims to project data points onto a lower-dimensional space while maximizing the separation between multiple classes. It achieves this by finding the linear combinations of features that best separate the classes. LDA assumes that the different classes generate data based on Gaussian distributions with the same covariance matrix.\n",
    "\n",
    "#### Key Steps\n",
    "1. Compute the Mean Vectors: Compute the mean vector for each class.\n",
    "2. Compute the Scatter Matrices:\n",
    "   - Within-Class Scatter Matrix: Measures the scatter (spread) of features within each class.\n",
    "   - Between-Class Scatter Matrix: Measures the scatter of the means of each class.\n",
    "3. Solve the Generalized Eigenvalue Problem: Compute the eigenvalues and eigenvectors for the scatter matrices to find the linear discriminants.\n",
    "4. Sort and Select Linear Discriminants: Sort the eigenvalues in descending order and select the top eigenvectors to form a matrix of linear discriminants.\n",
    "5. Project the Data: Transform the original data onto the new subspace using the matrix of linear discriminants.\n",
    "\n",
    "#### Implementation\n",
    "\n",
    "Suppose we have the Iris dataset and we want to classify it using Linear Discriminant Analysis.\n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Create and train the LDA model\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions\n",
    "y_pred = lda.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "print(f\"Classification Report:\\n{class_report}\")\n",
    "\n",
    "# Transforming the data for visualization\n",
    "X_lda = lda.transform(X)\n",
    "\n",
    "# Plotting the LDA result\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=X_lda[:, 0], y=X_lda[:, 1], hue=iris.target_names[y], palette='Set1')\n",
    "plt.title('LDA of Iris Dataset')\n",
    "plt.xlabel('LDA Component 1')\n",
    "plt.ylabel('LDA Component 2')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "#### Explanation \n",
    "\n",
    "1. Libraries: We import necessary libraries like numpy, pandas, sklearn, matplotlib, and seaborn.\n",
    "2. Data Preparation: We load the Iris dataset with four features and the target variable (species).\n",
    "3. Train-Test Split: We split the data into training and testing sets.\n",
    "4. Model Training: We create a LinearDiscriminantAnalysis model and train it using the training data.\n",
    "5. Predictions: We use the trained LDA model to predict the species of iris flowers for the test set.\n",
    "6. Evaluation:\n",
    "    - Accuracy: Measures the proportion of correctly classified instances.\n",
    "    - Confusion Matrix: Shows the counts of true positive, true negative, false positive, and false negative predictions.\n",
    "    - Classification Report: Provides precision, recall, F1-score, and support for each class.\n",
    "7. Transforming the Data: We project the data onto the new LDA components for visualization.\n",
    "    - Visualization: We create a scatter plot of the transformed data to visualize the separation of classes in the new subspace."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e7cd3e-c1ee-463a-87b6-4789df7c9636",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Day 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbeb524-3907-48be-ad3e-c59b32b66d3b",
   "metadata": {},
   "source": [
    "Let's start with Day 15 today \n",
    "\n",
    "Let's learn about XGBoost today \n",
    "\n",
    "Concept: XGBoost (Extreme Gradient Boosting) is an advanced implementation of gradient boosting designed for speed and performance. It builds an ensemble of decision trees sequentially, where each tree corrects the errors of its predecessor. XGBoost is known for its scalability, efficiency, and flexibility, and is widely used in machine learning competitions and real-world applications.\n",
    "\n",
    "#### Key Features of XGBoost\n",
    "1. Regularization: Helps prevent overfitting by penalizing complex models.\n",
    "2. Parallel Processing: Speeds up training by utilizing multiple cores of a CPU.\n",
    "3. Handling Missing Values: Automatically handles missing data by learning which path to take in a tree.\n",
    "4. Tree Pruning: Uses a depth-first approach to prune trees more effectively.\n",
    "5. Built-in Cross-Validation: Integrates cross-validation to optimize the number of boosting rounds.\n",
    "\n",
    "#### Key Steps\n",
    "1. Define the Objective Function: This is the loss function to be minimized.\n",
    "2. Compute Gradients: Calculate the gradients of the loss function.\n",
    "3. Fit the Trees: Train decision trees to predict the gradients.\n",
    "4. Update the Model: Combine the predictions of all trees to make the final prediction.\n",
    "\n",
    "#### Implementation\n",
    "\n",
    "Let's implement XGBoost using a common dataset like the Breast Cancer dataset from sklearn.\n",
    "\n",
    "##### Example\n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import xgboost as xgb\n",
    "\n",
    "# Load the Breast Cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the XGBoost model\n",
    "model = xgb.XGBClassifier(objective='binary:logistic', use_label_encoder=False)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "print(f\"Classification Report:\\n{class_report}\")\n",
    "```\n",
    "#### Explanation of the Code\n",
    "\n",
    "1. Libraries: We import necessary libraries like numpy, pandas, sklearn, and xgboost.\n",
    "2. Data Preparation: We load the Breast Cancer dataset with features and the target variable (malignant or benign).\n",
    "3. Train-Test Split: We split the data into training and testing sets.\n",
    "4. Model Training: We create an XGBClassifier model and train it using the training data.\n",
    "5. Predictions: We use the trained XGBoost model to predict the labels for the test set.\n",
    "6. Evaluation:\n",
    "    - Accuracy: Measures the proportion of correctly classified instances.\n",
    "    - Confusion Matrix: Shows the counts of true positive, true negative, false positive, and false negative predictions.\n",
    "    - Classification Report: Provides precision, recall, F1-score, and support for each class.\n",
    "\n",
    "```python\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "print(f\"Classification Report:\\n{class_report}\")\n",
    "```\n",
    "\n",
    "#### Applications\n",
    "\n",
    "XGBoost is widely used in various fields such as:\n",
    "- Finance: Fraud detection, credit scoring.\n",
    "- Healthcare: Disease prediction, patient risk stratification.\n",
    "- Marketing: Customer segmentation, churn prediction.\n",
    "- Sports: Player performance prediction, match outcome prediction.\n",
    "\n",
    "XGBoost's efficiency, accuracy, and versatility make it a top choice for many machine learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c2ec1b-f0b7-4d18-ace7-4f7ce0d74112",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Day 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae347613-96e9-4ff4-9b13-be8746252e37",
   "metadata": {},
   "source": [
    "Let's start with Day 16 today \n",
    "\n",
    "Let's learn about LightGBM algorithm \n",
    "\n",
    "#### Concept\n",
    "LightGBM (Light Gradient Boosting Machine) is a gradient boosting framework that uses tree-based learning algorithms. It is designed to be efficient and scalable, offering faster training speeds and higher efficiency compared to other gradient boosting algorithms. LightGBM handles large-scale data and offers better accuracy while consuming less memory.\n",
    "\n",
    "#### Key Features of LightGBM\n",
    "1. Leaf-Wise Tree Growth: Unlike level-wise growth used by other algorithms, LightGBM grows trees leaf-wise, focusing on the leaves with the maximum loss reduction.\n",
    "2. Histogram-Based Decision Tree: Uses a histogram-based algorithm to speed up training and reduce memory usage.\n",
    "3. Categorical Feature Support: Efficiently handles categorical features without needing to preprocess them.\n",
    "4. Optimal Split for Missing Values: Automatically handles missing values and determines the optimal split for them.\n",
    "\n",
    "#### Key Steps\n",
    "1. Define the Objective Function: The loss function to be minimized.\n",
    "2. Compute Gradients: Calculate the gradients of the loss function.\n",
    "3. Fit the Trees: Train decision trees to predict the gradients.\n",
    "4. Update the Model: Combine the predictions of all trees to make the final prediction.\n",
    "\n",
    "#### Implementation\n",
    "\n",
    "Let's implement LightGBM using the same Breast Cancer dataset for consistency.\n",
    "\n",
    "##### Example\n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Load the Breast Cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the LightGBM model\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'metric': 'binary_logloss',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9\n",
    "}\n",
    "\n",
    "# Train the model\n",
    "model = lgb.train(params, train_data, num_boost_round=100)\n",
    "\n",
    "# Making predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_binary = [1 if x > 0.5 else 0 for x in y_pred]\n",
    "\n",
    "# Evaluating the model\n",
    "accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_binary)\n",
    "class_report = classification_report(y_test, y_pred_binary)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "print(f\"Classification Report:\\n{class_report}\")\n",
    "```\n",
    "\n",
    "#### Explanation of the Code\n",
    "\n",
    "1. Libraries: We import necessary libraries like numpy, pandas, sklearn, and lightgbm.\n",
    "2. Data Preparation: We load the Breast Cancer dataset with features and the target variable (malignant or benign).\n",
    "3. Train-Test Split: We split the data into training and testing sets.\n",
    "4. Model Training: We create a LightGBM dataset and set the parameters for the model.\n",
    "5. Predictions: We use the trained LightGBM model to predict the labels for the test set.\n",
    "6. Evaluation:\n",
    "    - Accuracy: Measures the proportion of correctly classified instances.\n",
    "    - Confusion Matrix: Shows the counts of true positive, true negative, false positive, and false negative predictions.\n",
    "    - Classification Report: Provides precision, recall, F1-score, and support for each class.\n",
    "\n",
    "```python\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "print(f\"Classification Report:\\n{class_report}\")\n",
    "```\n",
    "\n",
    "#### Applications\n",
    "\n",
    "LightGBM is widely used in various fields such as:\n",
    "- Finance: Fraud detection, credit scoring.\n",
    "- Healthcare: Disease prediction, patient risk stratification.\n",
    "- Marketing: Customer segmentation, churn prediction.\n",
    "- Sports: Player performance prediction, match outcome prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d26188-509f-48fe-8201-56586056c920",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Day 17"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb526fd-c6c0-49ed-8b9b-353a85cdf4e1",
   "metadata": {},
   "source": [
    "Let's start with Day 17 today \n",
    "\n",
    "30 Days of Data Science Series: https://t.me/datasciencefun/1708\n",
    "\n",
    "Let's learn about CatBoost Algorithm \n",
    "\n",
    "Concept: CatBoost (Categorical Boosting) is a gradient boosting library that is particularly effective for datasets that include categorical features. It is designed to handle categorical data natively without the need for extensive preprocessing, such as one-hot encoding, which can lead to better performance and ease of use. \n",
    "\n",
    "#### Key Features of CatBoost\n",
    "1. Handling Categorical Features: Uses ordered boosting and a special technique to handle categorical features without needing preprocessing.\n",
    "2. Ordered Boosting: A technique to reduce overfitting by processing data in a specific order.\n",
    "3. Symmetric Trees: Ensures efficient memory usage and faster predictions by growing trees symmetrically.\n",
    "4. Robust to Overfitting: Incorporates techniques to minimize overfitting, making it suitable for various types of data.\n",
    "5. Efficient GPU Training: Supports fast training on GPU, which can significantly reduce training time.\n",
    "\n",
    "#### Key Steps\n",
    "1. Define the Objective Function: The loss function to be minimized.\n",
    "2. Compute Gradients: Calculate the gradients of the loss function.\n",
    "3. Fit the Trees: Train decision trees to predict the gradients.\n",
    "4. Update the Model: Combine the predictions of all trees to make the final prediction.\n",
    "\n",
    "#### Implementation\n",
    "\n",
    "Let's implement CatBoost using the same Breast Cancer dataset for consistency.\n",
    "\n",
    "##### Example\n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Load the Breast Cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the CatBoost model\n",
    "model = CatBoostClassifier(iterations=1000, learning_rate=0.1, depth=6, verbose=0)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "print(f\"Classification Report:\\n{class_report}\")\n",
    "```\n",
    "\n",
    "#### Explanation of the Code\n",
    "\n",
    "1. Libraries: We import necessary libraries like numpy, pandas, sklearn, and catboost.\n",
    "2. Data Preparation: We load the Breast Cancer dataset with features and the target variable (malignant or benign).\n",
    "3. Train-Test Split: We split the data into training and testing sets.\n",
    "4. Model Training: We create a CatBoostClassifier model and set the parameters for training.\n",
    "5. Predictions: We use the trained CatBoost model to predict the labels for the test set.\n",
    "6. Evaluation:\n",
    "    - Accuracy: Measures the proportion of correctly classified instances.\n",
    "    - Confusion Matrix: Shows the counts of true positive, true negative, false positive, and false negative predictions.\n",
    "    - Classification Report: Provides precision, recall, F1-score, and support for each class.\n",
    "\n",
    "```python\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "print(f\"Classification Report:\\n{class_report}\")\n",
    "```\n",
    "\n",
    "#### Applications\n",
    "\n",
    "CatBoost is widely used in various fields such as:\n",
    "- Finance: Fraud detection, credit scoring.\n",
    "- Healthcare: Disease prediction, patient risk stratification.\n",
    "- Marketing: Customer segmentation, churn prediction.\n",
    "- E-commerce: Product recommendation, customer behavior analysis.\n",
    "\n",
    "CatBoost's ability to handle categorical data efficiently and its robustness make it an excellent choice for many machine learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96df5c1-7766-41ef-861c-5fde810f04f2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Day 18"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88fd3e0-23bf-4552-aea1-d3de4351d28f",
   "metadata": {},
   "source": [
    "Let's start with Day 18 today \n",
    "\n",
    "Let's learn about Neural Networks\n",
    "\n",
    "#### Concept\n",
    "Neural Networks are a set of algorithms, modeled loosely after the human brain, designed to recognize patterns. They interpret sensory data through a kind of machine perception, labeling, or clustering of raw input. The patterns they recognize are numerical, contained in vectors, into which all real-world data, be it images, sound, text, or time series, must be translated.\n",
    "\n",
    "#### Key Features of Neural Networks\n",
    "1. Layers: Composed of an input layer, hidden layers, and an output layer.\n",
    "2. Neurons: Basic units that take inputs, apply weights, add a bias, and pass through an activation function.\n",
    "3. Activation Functions: Functions applied to the neurons' output, introducing non-linearity (e.g., ReLU, sigmoid, tanh).\n",
    "4. Backpropagation: Learning algorithm for training the network by minimizing the error.\n",
    "5. Training: Adjusts weights based on the error calculated from the output and the expected output.\n",
    "\n",
    "#### Key Steps\n",
    "1. Initialize Weights and Biases: Start with small random values.\n",
    "2. Forward Propagation: Pass inputs through the network layers to get predictions.\n",
    "3. Calculate Loss: Measure the difference between predictions and actual values.\n",
    "4. Backward Propagation: Compute the gradient of the loss function and update weights.\n",
    "5. Iteration: Repeat forward and backward propagation for a set number of epochs or until the loss converges.\n",
    "\n",
    "#### Implementation\n",
    "\n",
    "Let's implement a simple Neural Network using Keras on the Breast Cancer dataset.\n",
    "\n",
    "##### Example\n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Load the Breast Cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardizing the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Creating the Neural Network model\n",
    "model = Sequential([\n",
    "    Dense(30, input_shape=(X_train.shape[1],), activation='relu'),\n",
    "    Dense(15, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compiling the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training the model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=10, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Making predictions\n",
    "y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "\n",
    "# Evaluating the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "print(f\"Classification Report:\\n{class_report}\")\n",
    "```\n",
    "\n",
    "#### Explanation of the Code\n",
    "\n",
    "1. Libraries: We import necessary libraries like numpy, sklearn, and tensorflow.keras.\n",
    "2. Data Preparation: We load the Breast Cancer dataset with features and the target variable (malignant or benign).\n",
    "3. Train-Test Split: We split the data into training and testing sets.\n",
    "4. Data Standardization: We standardize the data for better convergence of the neural network.\n",
    "5. Model Creation: We create a sequential neural network with an input layer, two hidden layers, and an output layer.\n",
    "6. Model Compilation: We compile the model with the Adam optimizer and binary cross-entropy loss function.\n",
    "7. Model Training: We train the model for 50 epochs with a batch size of 10 and validate on 20% of the training data.\n",
    "8. Predictions: We make predictions on the test set and convert them to binary values.\n",
    "9. Evaluation:\n",
    "    - Accuracy: Measures the proportion of correctly classified instances.\n",
    "    - Confusion Matrix: Shows the counts of true positive, true negative, false positive, and false negative predictions.\n",
    "    - Classification Report: Provides precision, recall, F1-score, and support for each class.\n",
    "\n",
    "```python\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "print(f\"Classification Report:\\n{class_report}\")\n",
    "```\n",
    "\n",
    "#### Advanced Features of Neural Networks\n",
    "\n",
    "1. Hyperparameter Tuning: Tuning the number of layers, neurons, learning rate, batch size, and epochs for optimal performance.\n",
    "2. Regularization Techniques: \n",
    "   - Dropout: Randomly drops neurons during training to prevent overfitting.\n",
    "   - L1/L2 Regularization: Adds penalties to the loss function for large weights to prevent overfitting.\n",
    "3. Early Stopping: Stops training when the validation loss stops improving.\n",
    "4. Batch Normalization: Normalizes inputs of each layer to stabilize and accelerate training.\n",
    "\n",
    "```python\n",
    "# Example with Dropout and Batch Normalization\n",
    "from tensorflow.keras.layers import Dropout, BatchNormalization\n",
    "\n",
    "model = Sequential([\n",
    "Â Â Â  Dense(30, input_shape=(X_train.shape[1],), activation='relu'),\n",
    "Â Â Â  BatchNormalization(),\n",
    "Â Â Â  Dropout(0.5),\n",
    "Â Â Â  Dense(15, activation='relu'),\n",
    "Â Â Â  BatchNormalization(),\n",
    "Â Â Â  Dropout(0.5),\n",
    "Â Â Â  Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compiling and training remain the same as before\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=10, validation_split=0.2, verbose=1)\n",
    "```\n",
    "\n",
    "#### Applications\n",
    "\n",
    "Neural Networks are widely used in various fields such as:\n",
    "- Computer Vision: Image classification, object detection, facial recognition.\n",
    "- Natural Language Processing: Sentiment analysis, language translation, text generation.\n",
    "- Healthcare: Disease prediction, medical image analysis, drug discovery.\n",
    "- Finance: Stock price prediction, fraud detection, credit scoring.\n",
    "- Robotics: Autonomous driving, robotic control, gesture recognition.\n",
    "\n",
    "Neural Networks' ability to learn from data and recognize complex patterns makes them suitable for a wide range of applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68dafdf-6fca-4582-8212-b3ece2c34cdf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Day 19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2c292c-8a1b-42f3-865b-ded0bb992c23",
   "metadata": {},
   "source": [
    "Let's start with Day 19 today \n",
    "\n",
    "Let's learn about Convolutional Neural Networks (CNNs)\n",
    "\n",
    "#### Concept\n",
    "Convolutional Neural Networks (CNNs) are specialized neural networks designed to process data with a grid-like topology, such as images. They are particularly effective for image recognition and classification tasks due to their ability to capture spatial hierarchies in the data.\n",
    "\n",
    "#### Key Features of CNNs\n",
    "1. Convolutional Layers: Apply convolution operations to extract features from the input data.\n",
    "2. Pooling Layers: Reduce the dimensionality of the data while retaining important features.\n",
    "3. Fully Connected Layers: Perform classification based on the extracted features.\n",
    "4. Activation Functions: Introduce non-linearity to the network (e.g., ReLU).\n",
    "5. Filters/Kernels: Learnable parameters that detect specific patterns like edges, textures, etc.\n",
    "\n",
    "#### Key Steps\n",
    "1. Convolution Operation: Slide filters over the input image to create feature maps.\n",
    "2. Pooling Operation: Downsample the feature maps to reduce dimensions and computation.\n",
    "3. Flattening: Convert the 2D feature maps into a 1D vector for the fully connected layers.\n",
    "4. Fully Connected Layers: Perform the final classification based on the extracted features.\n",
    "\n",
    "#### Implementation\n",
    "\n",
    "Let's implement a simple CNN using Keras on the MNIST dataset, which consists of handwritten digit images.\n",
    "\n",
    "##### Example\n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Preprocessing the data\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype('float32') / 255\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1).astype('float32') / 255\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# Creating the CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compiling the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=200, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Evaluating the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "```\n",
    "\n",
    "#### Explanation of the Code\n",
    "\n",
    "1. Libraries: We import necessary libraries like numpy and tensorflow.keras.\n",
    "2. Data Loading: We load the MNIST dataset with images of handwritten digits.\n",
    "3. Data Preprocessing:\n",
    "   - Reshape the images to include a single channel (grayscale).\n",
    "   - Normalize pixel values to the range [0, 1].\n",
    "   - Convert the labels to one-hot encoded format.\n",
    "4. Model Creation:\n",
    "   - Conv2D Layers: Apply 32 and 64 filters with a kernel size of (3, 3) for feature extraction.\n",
    "   - MaxPooling2D Layers: Reduce the spatial dimensions of the feature maps.\n",
    "   - Flatten Layer: Convert 2D feature maps to a 1D vector.\n",
    "   - Dense Layers: Perform classification with 128 neurons in the hidden layer and 10 neurons in the output layer (one for each digit class).\n",
    "5. Model Compilation: We compile the model with the Adam optimizer and categorical cross-entropy loss function.\n",
    "6. Model Training: We train the model for 10 epochs with a batch size of 200 and validate on 20% of the training data.\n",
    "7. Model Evaluation: We evaluate the model on the test set and print the accuracy.\n",
    "\n",
    "```python\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "```\n",
    "\n",
    "#### Advanced Features of CNNs\n",
    "\n",
    "1. Deeper Architectures: Increase the number of convolutional and pooling layers for better feature extraction.\n",
    "2. Data Augmentation: Enhance the training set by applying transformations like rotation, flipping, and scaling.\n",
    "3. Transfer Learning: Use pre-trained models (e.g., VGG, ResNet) and fine-tune them on specific tasks.\n",
    "4. Regularization Techniques: \n",
    "   - Dropout: Randomly drop neurons during training to prevent overfitting.\n",
    "   - Batch Normalization: Normalize inputs of each layer to stabilize and accelerate training.\n",
    "\n",
    "```python\n",
    "# Example with Data Augmentation and Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "# Data Augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "Â Â Â  rotation_range=10,\n",
    "Â Â Â  zoom_range=0.1,\n",
    "Â Â Â  width_shift_range=0.1,\n",
    "Â Â Â  height_shift_range=0.1\n",
    ")\n",
    "\n",
    "# Creating the CNN model with Dropout\n",
    "model = Sequential([\n",
    "Â Â Â  Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "Â Â Â  MaxPooling2D(pool_size=(2, 2)),\n",
    "Â Â Â  Dropout(0.25),\n",
    "Â Â Â  Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
    "Â Â Â  MaxPooling2D(pool_size=(2, 2)),\n",
    "Â Â Â  Dropout(0.25),\n",
    "Â Â Â  Flatten(),\n",
    "Â Â Â  Dense(128, activation='relu'),\n",
    "Â Â Â  Dropout(0.5),\n",
    "Â Â Â  Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compiling and training remain the same as before\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(datagen.flow(X_train, y_train, batch_size=200), epochs=10, validation_data=(X_test, y_test), verbose=1)\n",
    "```\n",
    "\n",
    "#### Applications\n",
    "\n",
    "CNNs are widely used in various fields such as:\n",
    "- Computer Vision: Image classification, object detection, facial recognition.\n",
    "- Medical Imaging: Tumor detection, medical image segmentation.\n",
    "- Autonomous Driving: Road sign recognition, obstacle detection.\n",
    "- Augmented Reality: Gesture recognition, object tracking.\n",
    "- Security: Surveillance, biometric authentication.\n",
    "\n",
    "CNNs' ability to automatically learn hierarchical feature representations makes them highly effective for image-related tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b6bf35-2d35-4e3a-8966-333d6e5b2fe7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Day 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40b2822-5e6a-46ca-abd8-94a3a4a8ce7d",
   "metadata": {},
   "source": [
    "Let's start with Day 20 today \n",
    "\n",
    "Let's learn about Recurrent Neural Networks (RNNs)\n",
    "\n",
    "#### Concept\n",
    "Recurrent Neural Networks (RNNs) are a class of neural networks designed to recognize patterns in sequences of data such as time series, natural language, or video frames. Unlike traditional neural networks, RNNs have connections that form directed cycles, allowing them to maintain a hidden state that can capture information about previous inputs.\n",
    "\n",
    "#### Key Features of RNNs\n",
    "1. Sequential Data Processing: Designed to handle sequences of varying lengths.\n",
    "2. Hidden State: Maintains information about previous elements in the sequence.\n",
    "3. Shared Weights: Uses the same weights across all time steps, reducing the number of parameters.\n",
    "4. Vanishing/Exploding Gradient Problem: Can struggle with long-term dependencies due to these issues.\n",
    "\n",
    "#### Key Steps\n",
    "1. Input and Hidden States: Each input element is processed along with the hidden state from the previous time step.\n",
    "2. Recurrent Connections: The hidden state is updated recursively.\n",
    "3. Output Layer: Produces predictions based on the hidden state at each time step.\n",
    "\n",
    "#### Implementation\n",
    "\n",
    "Let's implement a simple RNN using Keras to predict the next value in a sequence of numbers.\n",
    "\n",
    "##### Example\n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Generate synthetic sequential data\n",
    "data = np.sin(np.linspace(0, 100, 1000))\n",
    "\n",
    "# Prepare the dataset\n",
    "def create_dataset(data, time_step=1):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - time_step - 1):\n",
    "        a = data[i:(i + time_step)]\n",
    "        X.append(a)\n",
    "        y.append(data[i + time_step])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Scale the data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "data = scaler.fit_transform(data.reshape(-1, 1))\n",
    "\n",
    "# Create the dataset with time steps\n",
    "time_step = 10\n",
    "X, y = create_dataset(data, time_step)\n",
    "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train_size = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "# Create the RNN model\n",
    "model = Sequential([\n",
    "    SimpleRNN(50, input_shape=(time_step, 1)),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=1, verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Loss: {loss}\")\n",
    "\n",
    "# Predict the next value in the sequence\n",
    "last_sequence = X_test[-1].reshape(1, time_step, 1)\n",
    "predicted_value = model.predict(last_sequence)\n",
    "predicted_value = scaler.inverse_transform(predicted_value)\n",
    "print(f\"Predicted Value: {predicted_value[0][0]}\")\n",
    "```\n",
    "\n",
    "#### Explanation of the Code\n",
    "\n",
    "1. Data Generation: We generate synthetic sequential data using a sine function.\n",
    "2. Dataset Preparation: We create sequences of 10 time steps to predict the next value.\n",
    "3. Data Scaling: Normalize the data to the range [0, 1] using MinMaxScaler.\n",
    "4. Dataset Creation: Create the dataset with input sequences and corresponding labels.\n",
    "5. Train-Test Split: Split the data into training and test sets.\n",
    "6. Model Creation:\n",
    "   - SimpleRNN Layer: A recurrent layer with 50 units.\n",
    "   - Dense Layer: A fully connected layer with a single output neuron for regression.\n",
    "7. Model Compilation: We compile the model with the Adam optimizer and mean squared error loss function.\n",
    "8. Model Training: Train the model for 50 epochs with a batch size of 1.\n",
    "9. Model Evaluation: Evaluate the model on the test set and print the loss.\n",
    "10. Prediction: Predict the next value in the sequence using the last sequence from the test set.\n",
    "\n",
    "```python\n",
    "print(f\"Predicted Value: {predicted_value[0][0]}\")\n",
    "```\n",
    "\n",
    "#### Advanced Features of RNNs\n",
    "\n",
    "1. LSTM (Long Short-Term Memory): Designed to handle long-term dependencies better than vanilla RNNs.\n",
    "2. GRU (Gated Recurrent Unit): A simplified version of LSTM with similar performance.\n",
    "3. Bidirectional RNNs: Process the sequence in both forward and backward directions.\n",
    "4. Stacked RNNs: Use multiple layers of RNNs for better feature extraction.\n",
    "5. Attention Mechanisms: Improve the model's ability to focus on important parts of the sequence.\n",
    "\n",
    "```python\n",
    "# Example with LSTM\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "# Create the LSTM model\n",
    "model = Sequential([\n",
    "Â Â Â  LSTM(50, input_shape=(time_step, 1)),\n",
    "Â Â Â  Dense(1)\n",
    "])\n",
    "\n",
    "# Compile, train, and evaluate the model (same as before)\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=1, verbose=1)\n",
    "loss = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Loss: {loss}\")\n",
    "```\n",
    "\n",
    "#### Applications\n",
    "\n",
    "RNNs are widely used in various fields such as:\n",
    "- Natural Language Processing (NLP): Language modeling, machine translation, text generation.\n",
    "- Time Series Analysis: Stock price prediction, weather forecasting, anomaly detection.\n",
    "- Speech Recognition: Transcribing spoken language into text.\n",
    "- Video Analysis: Activity recognition, video captioning.\n",
    "- Music Generation: Composing music by predicting sequences of notes.\n",
    "\n",
    "RNNs' ability to capture temporal dependencies makes them highly effective for sequential data tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392ef975-d1ea-46a4-8d14-764b1360600b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Day 21"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e207fa-254a-4fcb-b974-d32c3c10df5b",
   "metadata": {},
   "source": [
    "Let's start with Day 21 today \n",
    "\n",
    "Let's learn about Long Short-Term Memory (LSTM)\n",
    "\n",
    "#### Concept\n",
    "Long Short-Term Memory (LSTM) is a special type of Recurrent Neural Network (RNN) designed to overcome the limitations of traditional RNNs, specifically the vanishing and exploding gradient problems. LSTMs are capable of learning long-term dependencies, making them well-suited for tasks involving sequential data.\n",
    "\n",
    "#### Key Features of LSTM\n",
    "1. Memory Cell: Maintains information over long periods.\n",
    "2. Gates: Control the flow of information.\n",
    "   - Forget Gate: Decides what information to discard.\n",
    "   - Input Gate: Decides what new information to store.\n",
    "   - Output Gate: Decides what information to output.\n",
    "3. Cell State: Acts as a highway, carrying information across time steps.\n",
    "\n",
    "#### Key Steps\n",
    "1. Forget Gate: Uses a sigmoid function to decide which parts of the cell state to forget.\n",
    "2. Input Gate: Uses a sigmoid function to decide which parts of the new information to update.\n",
    "3. Cell State Update: Combines the old cell state and the new information.\n",
    "4. Output Gate: Uses a sigmoid function to decide what to output based on the updated cell state.\n",
    "\n",
    "#### Implementation\n",
    "\n",
    "Let's implement an LSTM for a sequence prediction problem using Keras.\n",
    "\n",
    "##### Example\n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Generate synthetic sequential data\n",
    "data = np.sin(np.linspace(0, 100, 1000))\n",
    "\n",
    "# Prepare the dataset\n",
    "def create_dataset(data, time_step=1):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - time_step - 1):\n",
    "        a = data[i:(i + time_step)]\n",
    "        X.append(a)\n",
    "        y.append(data[i + time_step])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Scale the data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "data = scaler.fit_transform(data.reshape(-1, 1))\n",
    "\n",
    "# Create the dataset with time steps\n",
    "time_step = 10\n",
    "X, y = create_dataset(data, time_step)\n",
    "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train_size = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "# Create the LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(50, input_shape=(time_step, 1)),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=1, verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Loss: {loss}\")\n",
    "\n",
    "# Predict the next value in the sequence\n",
    "last_sequence = X_test[-1].reshape(1, time_step, 1)\n",
    "predicted_value = model.predict(last_sequence)\n",
    "predicted_value = scaler.inverse_transform(predicted_value)\n",
    "print(f\"Predicted Value: {predicted_value[0][0]}\")\n",
    "```\n",
    "\n",
    "#### Explanation of the Code\n",
    "\n",
    "1. Data Generation: We generate synthetic sequential data using a sine function.\n",
    "2. Dataset Preparation: We create sequences of 10 time steps to predict the next value.\n",
    "3. Data Scaling: Normalize the data to the range [0, 1] using MinMaxScaler.\n",
    "4. Dataset Creation: Create the dataset with input sequences and corresponding labels.\n",
    "5. Train-Test Split: Split the data into training and test sets.\n",
    "6. Model Creation:\n",
    "   - LSTM Layer: An LSTM layer with 50 units.\n",
    "   - Dense Layer: A fully connected layer with a single output neuron for regression.\n",
    "7. Model Compilation: We compile the model with the Adam optimizer and mean squared error loss function.\n",
    "8. Model Training: Train the model for 50 epochs with a batch size of 1.\n",
    "9. Model Evaluation: Evaluate the model on the test set and print the loss.\n",
    "10. Prediction: Predict the next value in the sequence using the last sequence from the test set.\n",
    "\n",
    "```python\n",
    "print(f\"Predicted Value: {predicted_value[0][0]}\")\n",
    "```\n",
    "\n",
    "#### Advanced Features of LSTMs\n",
    "\n",
    "1. Bidirectional LSTM: Processes the sequence in both forward and backward directions.\n",
    "2. Stacked LSTM: Uses multiple LSTM layers to capture more complex patterns.\n",
    "3. Attention Mechanisms: Allows the model to focus on important parts of the sequence.\n",
    "4. Dropout Regularization: Prevents overfitting by randomly dropping units during training.\n",
    "5. Batch Normalization: Normalizes the inputs to each layer, improving training speed and stability.\n",
    "\n",
    "```python\n",
    "# Example with Stacked LSTM and Dropout\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "# Create the stacked LSTM model\n",
    "model = Sequential([\n",
    "Â Â Â  LSTM(50, return_sequences=True, input_shape=(time_step, 1)),\n",
    "Â Â Â  Dropout(0.2),\n",
    "Â Â Â  LSTM(50),\n",
    "Â Â Â  Dense(1)\n",
    "])\n",
    "\n",
    "# Compile, train, and evaluate the model (same as before)\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=1, verbose=1)\n",
    "loss = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Loss: {loss}\")\n",
    "```\n",
    "\n",
    "#### Applications\n",
    "\n",
    "LSTMs are widely used in various fields such as:\n",
    "- Natural Language Processing (NLP): Language modeling, machine translation, text generation.\n",
    "- Time Series Analysis: Stock price prediction, weather forecasting, anomaly detection.\n",
    "- Speech Recognition: Transcribing spoken language into text.\n",
    "- Video Analysis: Activity recognition, video captioning.\n",
    "- Music Generation: Composing music by predicting sequences of notes.\n",
    "\n",
    "LSTMs' ability to capture long-term dependencies makes them highly effective for sequential data tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7eb2d13-83ad-4539-9579-f73e59eb2997",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Day 22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3328f255-ba64-4883-bacb-fdb7386b5db0",
   "metadata": {},
   "source": [
    "Let's start with Day 22 today \n",
    "\n",
    "Let's learn about Gated Recurrent Units (GRU)\n",
    "\n",
    "#### Concept\n",
    "Gated Recurrent Units (GRUs) are a type of recurrent neural network (RNN) designed to handle the vanishing gradient problem that affects traditional RNNs. GRUs are similar to Long Short-Term Memory (LSTM) units but are simpler and have fewer parameters, making them computationally more efficient.\n",
    "\n",
    "#### Key Features of GRU\n",
    "1. Update Gate: Decides how much of the previous memory to keep.\n",
    "2. Reset Gate: Decides how much of the previous state to forget.\n",
    "3. Memory Cell: Combines the current input with the previous memory, controlled by the update and reset gates.\n",
    "\n",
    "#### Key Steps\n",
    "1. Reset Gate: Determines how to combine the new input with the previous memory.\n",
    "2. Update Gate: Determines the amount of previous memory to keep and combine with the new candidate state.\n",
    "3. New State Calculation: Combines the previous state and the new candidate state based on the update gate.\n",
    "\n",
    "#### Implementation\n",
    "\n",
    "Let's implement a GRU for a sequence prediction problem using Keras.\n",
    "\n",
    "##### Example\n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Generate synthetic sequential data\n",
    "data = np.sin(np.linspace(0, 100, 1000))\n",
    "\n",
    "# Prepare the dataset\n",
    "def create_dataset(data, time_step=1):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - time_step - 1):\n",
    "        a = data[i:(i + time_step)]\n",
    "        X.append(a)\n",
    "        y.append(data[i + time_step])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Scale the data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "data = scaler.fit_transform(data.reshape(-1, 1))\n",
    "\n",
    "# Create the dataset with time steps\n",
    "time_step = 10\n",
    "X, y = create_dataset(data, time_step)\n",
    "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train_size = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "# Create the GRU model\n",
    "model = Sequential([\n",
    "    GRU(50, input_shape=(time_step, 1)),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=1, verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Loss: {loss}\")\n",
    "\n",
    "# Predict the next value in the sequence\n",
    "last_sequence = X_test[-1].reshape(1, time_step, 1)\n",
    "predicted_value = model.predict(last_sequence)\n",
    "predicted_value = scaler.inverse_transform(predicted_value)\n",
    "print(f\"Predicted Value: {predicted_value[0][0]}\")\n",
    "```\n",
    "\n",
    "#### Explanation of the Code\n",
    "\n",
    "1. Data Generation: We generate synthetic sequential data using a sine function.\n",
    "2. Dataset Preparation: We create sequences of 10 time steps to predict the next value.\n",
    "3. Data Scaling: Normalize the data to the range [0, 1] using MinMaxScaler.\n",
    "4. Dataset Creation: Create the dataset with input sequences and corresponding labels.\n",
    "5. Train-Test Split: Split the data into training and test sets.\n",
    "6. Model Creation:\n",
    "   - GRU Layer: A GRU layer with 50 units.\n",
    "   - Dense Layer: A fully connected layer with a single output neuron for regression.\n",
    "7. Model Compilation: We compile the model with the Adam optimizer and mean squared error loss function.\n",
    "8. Model Training: Train the model for 50 epochs with a batch size of 1.\n",
    "9. Model Evaluation: Evaluate the model on the test set and print the loss.\n",
    "10. Prediction: Predict the next value in the sequence using the last sequence from the test set.\n",
    "\n",
    "#### Advanced Features of GRUs\n",
    "\n",
    "1. Bidirectional GRU: Processes the sequence in both forward and backward directions.\n",
    "2. Stacked GRU: Uses multiple GRU layers to capture more complex patterns.\n",
    "3. Attention Mechanisms: Allows the model to focus on important parts of the sequence.\n",
    "4. Dropout Regularization: Prevents overfitting by randomly dropping units during training.\n",
    "5. Batch Normalization: Normalizes the inputs to each layer, improving training speed and stability.\n",
    "\n",
    "```python\n",
    "# Example with Stacked GRU and Dropout\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "# Create the stacked GRU model\n",
    "model = Sequential([\n",
    "Â Â Â  GRU(50, return_sequences=True, input_shape=(time_step, 1)),\n",
    "Â Â Â  Dropout(0.2),\n",
    "Â Â Â  GRU(50),\n",
    "Â Â Â  Dense(1)\n",
    "])\n",
    "\n",
    "# Compile, train, and evaluate the model (same as before)\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=1, verbose=1)\n",
    "loss = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Loss: {loss}\")\n",
    "```\n",
    "\n",
    "#### Applications\n",
    "\n",
    "GRUs are widely used in various fields such as:\n",
    "- Natural Language Processing (NLP): Language modeling, machine translation, text generation.\n",
    "- Time Series Analysis: Stock price prediction, weather forecasting, anomaly detection.\n",
    "- Speech Recognition: Transcribing spoken language into text.\n",
    "- Video Analysis: Activity recognition, video captioning.\n",
    "- Music Generation: Composing music by predicting sequences of notes.\n",
    "\n",
    "GRUs' ability to capture long-term dependencies while being computationally efficient makes them a popular choice for sequential data tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17986998-a853-456c-9903-ba8d4ddef802",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Day 23"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed024bc-a47b-4505-bf06-d86fe2b647cb",
   "metadata": {},
   "source": [
    "Let's start with Day 23 today \n",
    "\n",
    "Let's learn about Autoencoders\n",
    "\n",
    "#### Concept\n",
    "Autoencoders are neural networks used for unsupervised learning tasks, particularly for dimensionality reduction and data compression. They learn to encode input data into a lower-dimensional representation (latent space) and then decode it back to the original data. The goal is to make the reconstructed data as close to the original as possible.\n",
    "\n",
    "#### Key Components\n",
    "1. Encoder: Maps the input data to a lower-dimensional space.\n",
    "2. Latent Space: The compressed representation of the input data.\n",
    "3. Decoder: Reconstructs the data from the lower-dimensional representation.\n",
    "\n",
    "#### Key Steps\n",
    "1. Encoding: Compress the input data into a latent space.\n",
    "2. Decoding: Reconstruct the input data from the latent space.\n",
    "3. Optimization: Minimize the reconstruction error between the original and the reconstructed data.\n",
    "\n",
    "#### Implementation\n",
    "\n",
    "Let's implement an autoencoder using Keras to compress and reconstruct images from the MNIST dataset.\n",
    "\n",
    "##### Example\n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(x_train, _), (x_test, _) = mnist.load_data()\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "\n",
    "# Define the autoencoder architecture\n",
    "input_dim = x_train.shape[1]\n",
    "encoding_dim = 32\n",
    "\n",
    "# Encoder\n",
    "input_img = Input(shape=(input_dim,))\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_img)\n",
    "\n",
    "# Decoder\n",
    "decoded = Dense(input_dim, activation='sigmoid')(encoded)\n",
    "\n",
    "# Autoencoder model\n",
    "autoencoder = Model(input_img, decoded)\n",
    "\n",
    "# Compile the model\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "# Train the model\n",
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=50,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test))\n",
    "\n",
    "# Encoder model to extract the latent representation\n",
    "encoder = Model(input_img, encoded)\n",
    "\n",
    "# Decoder model to reconstruct the input from the latent representation\n",
    "encoded_input = Input(shape=(encoding_dim,))\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "decoder = Model(encoded_input, decoder_layer(encoded_input))\n",
    "\n",
    "# Encode and decode some digits\n",
    "encoded_imgs = encoder.predict(x_test)\n",
    "decoded_imgs = decoder.predict(encoded_imgs)\n",
    "\n",
    "# Plot the original and reconstructed images\n",
    "n = 10\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # Display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # Display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "#### Explanation of the Code\n",
    "\n",
    "1. Data Preparation: Load the MNIST dataset, normalize the pixel values to the range [0, 1], and reshape the data.\n",
    "2. Autoencoder Architecture:\n",
    "   - Input Dimension: The dimension of the input data (784 for 28x28 images).\n",
    "   - Encoding Dimension: The size of the compressed representation (32 in this case).\n",
    "   - Encoder: A dense layer that compresses the input data to the encoding dimension.\n",
    "   - Decoder: A dense layer that reconstructs the input data from the compressed representation.\n",
    "3. Model Compilation: Compile the autoencoder model using the Adam optimizer and binary cross-entropy loss.\n",
    "4. Model Training: Train the model for 50 epochs with a batch size of 256, using the same data for input and output.\n",
    "5. Latent Representation and Reconstruction:\n",
    "   - Encoder Model: Extracts the latent representation from the input data.\n",
    "   - Decoder Model: Reconstructs the input data from the latent representation.\n",
    "6. Visualization: Display the original and reconstructed images to visually compare the results.\n",
    "\n",
    "#### Applications\n",
    "\n",
    "Autoencoders are used in various applications, including:\n",
    "\n",
    "1. Dimensionality Reduction: Reducing the number of features in high-dimensional data while preserving important information.\n",
    "2. Anomaly Detection: Identifying outliers or anomalies by measuring the reconstruction error.\n",
    "3. Denoising: Removing noise from data by training the autoencoder to reconstruct clean data from noisy inputs.\n",
    "4. Data Compression: Compressing data to save storage space or reduce transmission bandwidth.\n",
    "5. Image Generation: Generating new images by sampling from the latent space.\n",
    "\n",
    "#### Advanced Variants of Autoencoders\n",
    "\n",
    "1. Variational Autoencoders (VAEs): Introduce a probabilistic approach to learn a distribution over the latent space, enabling generation of new data samples.\n",
    "2. Denoising Autoencoders: Train the autoencoder to reconstruct clean data from noisy inputs, effectively learning to remove noise.\n",
    "3. Sparse Autoencoders: Encourage sparsity in the latent representation, making the model learn more robust features.\n",
    "4. Convolutional Autoencoders (CAEs): Use convolutional layers for encoding and decoding, making them more suitable for image data.\n",
    "5. Sequence-to-Sequence Autoencoders: Designed for sequential data, such as text or time series, using RNNs or LSTMs in the encoder and decoder.\n",
    "\n",
    "Autoencoders' versatility and ability to learn compact representations make them powerful tools for a wide range of unsupervised learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2dde80-ace6-45b9-9ebb-2540d4508c5d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Day 24"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a760ded-b74a-45ce-8232-c8690ca5f024",
   "metadata": {},
   "source": [
    "Let's start with Day 24 today \n",
    "\n",
    "Let's learn about Generative Adversarial Networks (GANs)\n",
    "\n",
    "Concept: Generative Adversarial Networks (GANs) are a type of deep learning framework introduced by Ian Goodfellow and colleagues in 2014. GANs are used for generating new data samples similar to a given dataset. They consist of two neural networks: a generator and a discriminator, which are trained simultaneously in a competitive manner.\n",
    "\n",
    "Key Components:\n",
    "\n",
    "1. Generator: Takes random noise as input and generates fake data samples.\n",
    "2. Discriminator: Takes both real and generated data samples as input and predicts whether the samples are real or fake.\n",
    "3. Adversarial Training: The generator and discriminator are trained alternately: the generator aims to fool the discriminator by generating realistic samples, while the discriminator learns to distinguish between real and fake samples.\n",
    "\n",
    "#### Key Steps\n",
    "1. Generator Training: Update the generator to minimize the discriminator's ability to distinguish between real and generated samples.\n",
    "2. Discriminator Training: Update the discriminator to better distinguish between real and generated samples.\n",
    "\n",
    "#### Implementation\n",
    "\n",
    "Let's implement a simple GAN using TensorFlow/Keras to generate handwritten digits similar to those in the MNIST dataset. ðð\n",
    "\n",
    "##### Example\n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.layers import Dense, Flatten, Reshape\n",
    "from tensorflow.keras.layers import LeakyReLU, BatchNormalization\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "# Normalize the data\n",
    "X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "X_train = X_train.reshape(X_train.shape[0], 784)\n",
    "\n",
    "# Define the generator model\n",
    "generator = Sequential([\n",
    "Â Â Â  Dense(256, input_dim=100),\n",
    "Â Â Â  LeakyReLU(alpha=0.2),\n",
    "Â Â Â  BatchNormalization(),\n",
    "Â Â Â  Dense(512),\n",
    "Â Â Â  LeakyReLU(alpha=0.2),\n",
    "Â Â Â  BatchNormalization(),\n",
    "Â Â Â  Dense(1024),\n",
    "Â Â Â  LeakyReLU(alpha=0.2),\n",
    "Â Â Â  BatchNormalization(),\n",
    "Â Â Â  Dense(784, activation='tanh'),\n",
    "Â Â Â  Reshape((28, 28))\n",
    "])\n",
    "\n",
    "# Define the discriminator model\n",
    "discriminator = Sequential([\n",
    "Â Â Â  Flatten(input_shape=(28, 28)),\n",
    "Â Â Â  Dense(1024),\n",
    "Â Â Â  LeakyReLU(alpha=0.2),\n",
    "Â Â Â  Dense(512),\n",
    "Â Â Â  LeakyReLU(alpha=0.2),\n",
    "Â Â Â  Dense(256),\n",
    "Â Â Â  LeakyReLU(alpha=0.2),\n",
    "Â Â Â  Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the discriminator\n",
    "discriminator.compile(optimizer=Adam(learning_rate=0.0002, beta_1=0.5),\n",
    "Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Compile the GAN model\n",
    "discriminator.trainable = False\n",
    "gan_input = Input(shape=(100,))\n",
    "x = generator(gan_input)\n",
    "gan_output = discriminator(x)\n",
    "gan = Model(gan_input, gan_output)\n",
    "gan.compile(optimizer=Adam(learning_rate=0.0002, beta_1=0.5),\n",
    "Â Â Â Â Â Â Â Â Â Â Â  loss='binary_crossentropy')\n",
    "\n",
    "# Function to train the GAN\n",
    "def train_gan(epochs=1, batch_size=128):\n",
    "Â Â Â  # Calculate the number of batches per epoch\n",
    "Â Â Â  batch_count = X_train.shape[0] // batch_size\n",
    "Â Â Â  \n",
    "Â Â Â  for e in range(epochs):\n",
    "Â Â Â Â Â Â Â  for _ in range(batch_count):\n",
    "Â Â Â Â Â Â Â Â Â Â Â  # Generate random noise as input for the generator\n",
    "Â Â Â Â Â Â Â Â Â Â Â  noise = np.random.normal(0, 1, size=[batch_size, 100])\n",
    "Â Â Â Â Â Â Â Â Â Â Â  \n",
    "Â Â Â Â Â Â Â Â Â Â Â  # Generate fake images using the generator\n",
    "Â Â Â Â Â Â Â Â Â Â Â  generated_images = generator.predict(noise)\n",
    "Â Â Â Â Â Â Â Â Â Â Â  \n",
    "Â Â Â Â Â Â Â Â Â Â Â  # Get a random batch of real images from the dataset\n",
    "Â Â Â Â Â Â Â Â Â Â Â  batch_idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "Â Â Â Â Â Â Â Â Â Â Â  real_images = X_train[batch_idx]\n",
    "Â Â Â Â Â Â Â Â Â Â Â  \n",
    "Â Â Â Â Â Â Â Â Â Â Â  # Concatenate real and fake images\n",
    "Â Â Â Â Â Â Â Â Â Â Â  X = np.concatenate([real_images, generated_images])\n",
    "Â Â Â Â Â Â Â Â Â Â Â  \n",
    "Â Â Â Â Â Â Â Â Â Â Â  # Labels for generated and real data\n",
    "Â Â Â Â Â Â Â Â Â Â Â  y_dis = np.zeros(2 * batch_size)\n",
    "Â Â Â Â Â Â Â Â Â Â Â  y_dis[:batch_size] = 0.9Â  # One-sided label smoothing\n",
    "Â Â Â Â Â Â Â Â Â Â Â  \n",
    "Â Â Â Â Â Â Â Â Â Â Â  # Train the discriminator\n",
    "Â Â Â Â Â Â Â Â Â Â Â  discriminator.trainable = True\n",
    "Â Â Â Â Â Â Â Â Â Â Â  d_loss = discriminator.train_on_batch(X, y_dis)\n",
    "Â Â Â Â Â Â Â Â Â Â Â  \n",
    "Â Â Â Â Â Â Â Â Â Â Â  # Train the generator (via the GAN model)\n",
    "Â Â Â Â Â Â Â Â Â Â Â  noise = np.random.normal(0, 1, size=[batch_size, 100])\n",
    "Â Â Â Â Â Â Â Â Â Â Â  y_gen = np.ones(batch_size)\n",
    "Â Â Â Â Â Â Â Â Â Â Â  discriminator.trainable = False\n",
    "Â Â Â Â Â Â Â Â Â Â Â  g_loss = gan.train_on_batch(noise, y_gen)\n",
    "Â Â Â Â Â Â Â Â Â Â Â  \n",
    "Â Â Â Â Â Â Â  # Print the progress and save the generated images\n",
    "Â Â Â Â Â Â Â  print(f\"Epoch {e+1}, Discriminator Loss: {d_loss[0]}, Generator Loss: {g_loss}\")\n",
    "Â Â Â Â Â Â Â  if e % 10 == 0:\n",
    "Â Â Â Â Â Â Â Â Â Â Â  plot_generated_images(e, generator)\n",
    "\n",
    "# Function to plot generated images\n",
    "def plot_generated_images(epoch, generator, examples=10, dim=(1, 10), figsize=(10, 1)):\n",
    "Â Â Â  noise = np.random.normal(0, 1, size=[examples, 100])\n",
    "Â Â Â  generated_images = generator.predict(noise)\n",
    "Â Â Â  generated_images = generated_images.reshape(examples, 28, 28)\n",
    "\n",
    "Â Â Â  plt.figure(figsize=figsize)\n",
    "Â Â Â  for i in range(examples):\n",
    "Â Â Â Â Â Â Â  plt.subplot(dim[0], dim[1], i+1)\n",
    "Â Â Â Â Â Â Â  plt.imshow(generated_images[i], interpolation='nearest', cmap='gray')\n",
    "Â Â Â Â Â Â Â  plt.axis('off')\n",
    "Â Â Â  plt.tight_layout()\n",
    "Â Â Â  plt.savefig(f'gan_generated_image_epoch_{epoch}.png')\n",
    "Â Â Â  plt.show()\n",
    "\n",
    "# Train the GAN\n",
    "train_gan(epochs=100, batch_size=128)\n",
    "```\n",
    "\n",
    "#### Explanation of the above Code\n",
    "\n",
    "1. Data Loading and Preprocessing: Load the MNIST dataset and normalize the pixel values to the range [-1, 1].\n",
    "2. Generator Model:\n",
    "   - Sequential model with several dense layers followed by batch normalization and LeakyReLU activation, ending with a tanh activation layer to generate fake images.\n",
    "3. Discriminator Model:\n",
    "   - Sequential model to classify real and fake images, using dense layers with LeakyReLU activation and a sigmoid output layer.\n",
    "4. GAN Model:\n",
    "   - Combined model where the generator takes random noise as input and produces fake images, and the discriminator is trained to distinguish between real and fake images.\n",
    "5. Training Loop:\n",
    "   - Alternately trains the discriminator and the generator on batches of real and fake images.\n",
    "   - The generator aims to fool the discriminator by generating realistic images, while the discriminator aims to correctly classify real and fake images.\n",
    "6. Image Generation:\n",
    "   - Periodically saves generated images to visualize the training progress.\n",
    "\n",
    "#### Applications\n",
    "\n",
    "Generative Adversarial Networks have applications in:\n",
    "- Image Generation: Generating realistic images of faces, objects, or scenes.\n",
    "- Data Augmentation: Creating new training examples to improve the performance of machine learning models.\n",
    "- Image Editing: Modifying existing images by changing specific attributes.\n",
    "- Text-to-Image Synthesis: Generating images based on textual descriptions.\n",
    "- Video Generation: Creating new video frames based on existing frames.\n",
    "\n",
    "GANs' ability to generate high-quality, realistic data has led to significant advancements in various fields, including computer vision, natural language processing, and biomedical imaging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844fef12-a770-4b68-a1c1-345a74606d8b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Day 25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb6215c-5002-496d-8d80-b00810977aa5",
   "metadata": {},
   "source": [
    "Let's start with Day 25 today \n",
    "\n",
    "Let's learn about Transfer Learning today \n",
    "\n",
    "#### Concept\n",
    "\n",
    "Transfer learning is a machine learning technique where a model trained on one task is re-purposed on a second related task. It leverages the knowledge gained from the source task to improve learning in the target task, especially when the target dataset is small or different from the source dataset.\n",
    "\n",
    "#### Key Aspects\n",
    "\n",
    "1. Pre-trained Models: Utilize models trained on large-scale datasets like ImageNet, which have learned rich feature representations from extensive data.\n",
    "   \n",
    "2. Fine-tuning: Adapt pre-trained models to new tasks by updating weights during training on the target dataset. Fine-tuning allows the model to adjust its learned representations to fit the new task better.\n",
    "\n",
    "3. Domain Adaptation: Adjusting a model trained on one distribution (source domain) to perform well on another distribution (target domain) with different characteristics.\n",
    "\n",
    "#### Implementation Steps\n",
    "\n",
    "1. Select a Pre-trained Model: Choose a model pre-trained on a large dataset relevant to your task (e.g., VGG, ResNet, BERT).\n",
    "\n",
    "2. Adaptation to New Task: \n",
    "   - Feature Extraction: Freeze most layers of the pre-trained model and extract features from intermediate layers for the new dataset.\n",
    "   - Fine-tuning: Fine-tune the entire model or only a few top layers on the new dataset with a lower learning rate to avoid overfitting.\n",
    "\n",
    "3. Evaluation: Evaluate the performance of the adapted model on the target task using appropriate metrics (e.g., accuracy, precision, recall).\n",
    "\n",
    "#### Example: Transfer Learning with Pre-trained CNN for Image Classification\n",
    "\n",
    "Let's demonstrate transfer learning using a pre-trained VGG16 model for classifying images from a new dataset (e.g., CIFAR-10).\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Preprocess the data\n",
    "X_train = X_train.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "\n",
    "# Load pre-trained VGG16 model (excluding top layers)\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n",
    "\n",
    "# Freeze the layers in base model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Create a new model on top of the pre-trained base model\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=128,\n",
    "                    validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f'Test accuracy: {test_acc}')\n",
    "\n",
    "# Fine-tuning the model\n",
    "for layer in base_model.layers[-4:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.00001),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=5, batch_size=128,\n",
    "                    validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the fine-tuned model\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f'Fine-tuned test accuracy: {test_acc}')\n",
    "```\n",
    "\n",
    "#### Explanation:\n",
    "\n",
    "1. Loading Data: Load and preprocess the CIFAR-10 dataset.\n",
    "2. Base Model: Load VGG16 pre-trained on ImageNet without the top layers.\n",
    "3. Model Construction: Add custom top layers (fully connected, dropout, output) to the pre-trained base.\n",
    "4. Training: Train the model on the CIFAR-10 dataset.\n",
    "5. Fine-tuning: Optionally, unfreeze a few top layers of the base model and continue training with a lower learning rate to adapt to the new task.\n",
    "6. Evaluation: Evaluate the final model's performance on the test set.\n",
    "\n",
    "#### Applications\n",
    "\n",
    "Transfer learning is widely used in:\n",
    "- Computer Vision: Image classification, object detection, and segmentation.\n",
    "- Natural Language Processing: Text classification, sentiment analysis, and language translation.\n",
    "- Audio Processing: Speech recognition and sound classification.\n",
    "\n",
    "#### Advantages\n",
    "\n",
    "- Reduced Training Time: Leveraging pre-trained models reduces the need for training from scratch.\n",
    "- Improved Performance: Transfer learning can improve model accuracy, especially with limited labeled data.\n",
    "- Broader Applicability: Models trained on diverse datasets can be adapted to various real-world applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846a06df-7e77-468c-95bc-34d6d6e13772",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Day 26"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698b06fc-e942-4350-b466-2a39bd601488",
   "metadata": {},
   "source": [
    "Let's start with Day 26 today \n",
    "\n",
    "Let's learn about Ensemble Learning\n",
    "\n",
    "Concept: Ensemble learning is a machine learning technique where multiple models (learners) are trained to solve the same problem and their predictions are combined to improve the overall performance. The idea behind ensemble methods is that by combining multiple models, each with its own strengths and weaknesses, the ensemble can achieve better predictive performance than any single model alone.\n",
    "\n",
    "#### Key Aspects\n",
    "\n",
    "1. Diversity in Models: Ensemble methods benefit from using models that make different types of errors or have different biases.\n",
    "   \n",
    "2. Aggregation Methods: Common techniques for combining predictions include averaging (for regression tasks) and voting (for classification tasks).\n",
    "\n",
    "3. Types of Ensemble Methods:\n",
    "   - Bagging (Bootstrap Aggregating): Training multiple models independently on different subsets of the training data and aggregating their predictions (e.g., Random Forest).\n",
    "   - Boosting: Sequentially train models where each subsequent model corrects the errors of the previous one (e.g., AdaBoost, Gradient Boosting Machines).\n",
    "   - Stacking: Combining multiple models using another model (meta-learner) to learn how to best combine their predictions.\n",
    "\n",
    "#### Implementation Steps\n",
    "\n",
    "1. Choose Base Learners: Select diverse base models (e.g., decision trees, SVMs, neural networks) that perform reasonably well on the task.\n",
    "\n",
    "2. Aggregate Predictions: Combine predictions from individual models using averaging, voting, or more sophisticated methods.\n",
    "\n",
    "3. Evaluate Ensemble Performance: Assess the ensemble's performance on validation or test data using appropriate metrics (e.g., accuracy, F1-score, RMSE).\n",
    "\n",
    "#### Example: Voting Classifier for Ensemble Learning\n",
    "\n",
    "Let's implement a simple voting classifier using scikit-learn for a classification task.\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define base classifiers\n",
    "clf1 = LogisticRegression(random_state=42)\n",
    "clf2 = DecisionTreeClassifier(random_state=42)\n",
    "clf3 = SVC(random_state=42)\n",
    "\n",
    "# Create a voting classifier\n",
    "voting_clf = VotingClassifier(estimators=[('lr', clf1), ('dt', clf2), ('svc', clf3)], voting='hard')\n",
    "\n",
    "# Train the voting classifier\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict using the voting classifier\n",
    "y_pred = voting_clf.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Voting Classifier Accuracy: {accuracy:.2f}')\n",
    "```\n",
    "\n",
    "#### Explanation:\n",
    "\n",
    "1. Loading Data: Load the Iris dataset, a classic dataset for classification tasks.\n",
    "   \n",
    "2. Base Classifiers: Define three different base classifiers: Logistic Regression, Decision Tree, and Support Vector Machine (SVM).\n",
    "\n",
    "3. Voting Classifier: Create a voting classifier that aggregates predictions using a majority voting strategy (voting='hard').\n",
    "\n",
    "4. Training and Prediction: Train the voting classifier on the training data and predict labels for the test data.\n",
    "\n",
    "5. Evaluation: Compute the accuracy score to evaluate the voting classifier's performance.\n",
    "\n",
    "#### Applications\n",
    "\n",
    "Ensemble learning is widely used in various domains, including:\n",
    "- Classification: Improving accuracy and robustness of classifiers.\n",
    "- Regression: Enhancing predictive performance by combining different models.\n",
    "- Anomaly Detection: Identifying outliers or unusual patterns in data.\n",
    "- Recommendation Systems: Aggregating predictions from multiple models for personalized recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d65fabb-d777-42d7-b1ce-848ed4860af3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Day 27"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a756dfd-380a-458a-ae01-29d56b6e015e",
   "metadata": {},
   "source": [
    "Let's start with Day 27 today \n",
    "\n",
    "Let's learn about Natural Language Processing (NLP)\n",
    "\n",
    "Concept: Natural Language Processing (NLP) is a field of artificial intelligence focused on enabling computers to understand, interpret, and generate human language in a way that is both valuable and meaningful. \n",
    "\n",
    "#### Key Aspects\n",
    "\n",
    "1. Text Preprocessing: Cleaning and transforming raw text data into a format suitable for analysis (e.g., tokenization, stemming, lemmatization).\n",
    "\n",
    "2. Feature Extraction: Converting text into numerical representations (e.g., Bag-of-Words, TF-IDF, word embeddings like Word2Vec or GloVe).\n",
    "\n",
    "3. NLP Tasks:\n",
    "   - Text Classification: Assigning predefined categories to text documents (e.g., sentiment analysis, spam detection).\n",
    "   - Named Entity Recognition (NER): Identifying and classifying named entities (e.g., person names, organizations) in text.\n",
    "   - Text Generation: Creating coherent and meaningful sentences or paragraphs based on input text.\n",
    "   - Machine Translation: Automatically translating text from one language to another.\n",
    "   - Question Answering: Generating answers to questions posed in natural language.\n",
    "\n",
    "Implementation Steps\n",
    "\n",
    "1. Data Acquisition: Obtain a dataset or corpus of text data relevant to the task at hand.\n",
    "\n",
    "2. Text Preprocessing: Clean and preprocess the text data to remove noise, normalize text, and prepare it for analysis.\n",
    "\n",
    "3. Feature Extraction: Select and implement appropriate techniques to convert text data into numerical features suitable for machine learning models.\n",
    "\n",
    "4. Model Selection: Choose and train models suitable for the specific NLP task (e.g., classifiers for text classification, sequence models for text generation).\n",
    "\n",
    "5. Evaluation: Evaluate the model's performance using relevant metrics (e.g., accuracy, F1-score for classification tasks) and validate results.\n",
    "\n",
    "#### Example: Text Classification with TF-IDF and SVM\n",
    "\n",
    "Let's implement a basic text classification pipeline using TF-IDF (Term Frequency-Inverse Document Frequency) for feature extraction and SVM (Support Vector Machine) for classification.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Example dataset (you can replace this with your own dataset)\n",
    "data = {\n",
    "    'text': [\"This movie is great!\", \"I didn't like this film.\", \"The performance was outstanding.\"],\n",
    "    'label': [1, 0, 1]  # Example labels (1 for positive, 0 for negative sentiment)\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)  # Limit to top 1000 features\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Initialize SVM classifier\n",
    "svm_clf = SVC(kernel='linear')\n",
    "\n",
    "# Train the SVM classifier\n",
    "svm_clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = svm_clf.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "```\n",
    "\n",
    "#### Explanation:\n",
    "\n",
    "1. Dataset: Use a small example dataset with text and corresponding sentiment labels (1 for positive, 0 for negative).\n",
    "\n",
    "2. TF-IDF Vectorization: Convert text data into numerical TF-IDF features using TfidfVectorizer.\n",
    "\n",
    "3. SVM Classifier: Implement a linear SVM classifier (SVC(kernel='linear')) for text classification.\n",
    "\n",
    "4. Training and Evaluation: Train the SVM model on the TF-IDF transformed training data and evaluate its performance on the test set using accuracy and a classification report.\n",
    "\n",
    "#### Applications\n",
    "\n",
    "NLP techniques are essential in various applications, including:\n",
    "- Sentiment Analysis: Analyzing opinions and emotions expressed in text.\n",
    "- Information Extraction: Identifying relevant information from text documents.\n",
    "- Chatbots and Virtual Assistants: Understanding and responding to human queries in natural language.\n",
    "- Document Summarization: Generating concise summaries of large text documents.\n",
    "- Language Translation: Translating text from one language to another automatically.\n",
    "\n",
    "#### Advantages\n",
    "\n",
    "- Automated Analysis: Allows machines to process and understand human language at scale.\n",
    "- Insight Extraction: Extracts valuable insights and information from unstructured text data.\n",
    "- Improves Efficiency: Automates tasks that would otherwise require human effort and time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f39939-1eaa-4f9b-9f43-f7a8eee1d42f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Day 28"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1b37a8-eafa-4d5e-a744-8f6c5e61fda2",
   "metadata": {},
   "source": [
    "Let's start with Day 28 today \n",
    "\n",
    "Let's learn about Time Series Analysis and Forecasting today\n",
    "\n",
    "Concept: Time Series Analysis involves analyzing data points collected over time to extract meaningful statistics and other characteristics of the data. Time series forecasting, on the other hand, aims to predict future values based on previously observed data points. This field is crucial for understanding trends, making informed decisions, and planning for the future based on historical data patterns.\n",
    "\n",
    "#### Key Aspects\n",
    "\n",
    "1. Components of Time Series:\n",
    "   - Trend: The long-term movement or direction of the series (e.g., increasing or decreasing).\n",
    "   - Seasonality: Regular, periodic fluctuations in the series (e.g., daily, weekly, or yearly patterns).\n",
    "   - Noise: Random variations or irregularities in the data that are not systematic.\n",
    "\n",
    "2. Common Time Series Techniques:\n",
    "   - Moving Average: Smooths out short-term fluctuations to identify trends.\n",
    "   - Exponential Smoothing: Assigns exponentially decreasing weights over time to prioritize recent data.\n",
    "   - ARIMA (AutoRegressive Integrated Moving Average): Models time series data to capture patterns in the data.\n",
    "   - Prophet: A forecasting tool developed by Facebook that handles daily, weekly, and yearly seasonality.\n",
    "   - Deep Learning Models: Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) networks for complex time series patterns.\n",
    "\n",
    "3. Evaluation Metrics:\n",
    "   - Mean Absolute Error (MAE): Average of the absolute differences between predicted and actual values.\n",
    "   - Mean Squared Error (MSE): Average of the squared differences between predicted and actual values.\n",
    "   - Root Mean Squared Error (RMSE): Square root of the MSE, which gives an idea of the magnitude of error.\n",
    "\n",
    "#### Implementation Steps\n",
    "\n",
    "1. Data Preparation: Obtain and preprocess time series data (e.g., handling missing values, ensuring time-based ordering).\n",
    "\n",
    "2. Exploratory Data Analysis (EDA): Visualize the time series to identify trends, seasonality, and outliers.\n",
    "\n",
    "3. Model Selection: Choose an appropriate technique based on the characteristics of the time series data (e.g., ARIMA for stationary data, Prophet for data with seasonality).\n",
    "\n",
    "4. Training and Testing: Split the data into training and testing sets. Train the model on the training data and evaluate its performance on the test data.\n",
    "\n",
    "5. Forecasting: Generate forecasts for future time points based on the trained model.\n",
    "\n",
    "#### Example: ARIMA Model for Time Series Forecasting\n",
    "\n",
    "Let's implement an ARIMA model using Python's statsmodels library to forecast future values of a time series dataset.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Example time series data (replace with your own dataset)\n",
    "np.random.seed(42)\n",
    "date_range = pd.date_range(start='1/1/2020', periods=365)\n",
    "data = pd.Series(np.random.randn(len(date_range)), index=date_range)\n",
    "\n",
    "# Plotting the time series data\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(data)\n",
    "plt.title('Example Time Series Data')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Fit ARIMA model\n",
    "model = ARIMA(data, order=(1, 1, 1))  # Example order, replace with appropriate values\n",
    "model_fit = model.fit()\n",
    "\n",
    "# Forecasting future values\n",
    "forecast_steps = 30  # Number of steps ahead to forecast\n",
    "forecast = model_fit.forecast(steps=forecast_steps)\n",
    "\n",
    "# Plotting the forecasts\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(data, label='Observed')\n",
    "plt.plot(forecast, label='Forecast', linestyle='--')\n",
    "plt.title('ARIMA Forecasting')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Evaluate forecast accuracy (example using RMSE)\n",
    "test_data = pd.Series(np.random.randn(forecast_steps))  # Example test data, replace with actual test data\n",
    "rmse = np.sqrt(mean_squared_error(test_data, forecast))\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse:.2f}')\n",
    "```\n",
    "\n",
    "#### Explanation:\n",
    "\n",
    "1. Data Generation: Generate synthetic time series data for demonstration purposes.\n",
    "\n",
    "2. Visualization: Plot the time series data to visualize trends and patterns.\n",
    "\n",
    "3. ARIMA Model: Initialize and fit an ARIMA model (order=(p, d, q)) to capture autocorrelations in the data.\n",
    "\n",
    "4. Forecasting: Forecast future values using the trained ARIMA model for a specified number of steps ahead.\n",
    "\n",
    "5. Evaluation: Evaluate the forecast accuracy using metrics such as RMSE.\n",
    "\n",
    "#### Applications\n",
    "\n",
    "Time series analysis and forecasting are applicable in various domains:\n",
    "- Finance: Predicting stock prices, market trends, and economic indicators.\n",
    "- Healthcare: Forecasting patient admissions, disease outbreaks, and resource planning.\n",
    "- Retail: Demand forecasting, inventory management, and sales predictions.\n",
    "- Energy: Load forecasting, optimizing energy consumption, and pricing strategies.\n",
    "\n",
    "#### Advantages\n",
    "\n",
    "- Data-Driven Insights: Provides insights into historical trends and future predictions based on data patterns.\n",
    "- Decision Support: Assists in making informed decisions and planning strategies.\n",
    "- Continuous Improvement: Models can be updated with new data to improve accuracy over time.\n",
    "\n",
    "Mastering time series analysis and forecasting enables data-driven decision-making and strategic planning based on historical data patterns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1696adfa-951f-4b4b-aeea-154ae879352e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Day 29"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff11f539-81f3-4bbc-bda8-584197479155",
   "metadata": {},
   "source": [
    "Let's start with Day 29 today \n",
    "\n",
    "Let's learn about Model Deployment and Monitoring today\n",
    "\n",
    "#### Concept\n",
    "\n",
    "Model Deployment and Monitoring involve the processes of making trained machine learning models accessible for use in production environments and continuously monitoring their performance and behavior to ensure they deliver reliable and accurate predictions.\n",
    "\n",
    "#### Key Aspects\n",
    "\n",
    "1. Model Deployment:\n",
    "   - Packaging: Prepare the model along with necessary dependencies (libraries, configurations).\n",
    "   - Scalability: Ensure the model can handle varying workloads and data volumes.\n",
    "   - Integration: Integrate the model into existing software systems or applications for seamless operation.\n",
    "\n",
    "2. Model Monitoring:\n",
    "   - Performance Metrics: Track metrics such as accuracy, precision, recall, and F1-score to assess model performance over time.\n",
    "   - Data Drift Detection: Monitor changes in input data distributions that may affect model performance.\n",
    "   - Model Drift Detection: Identify changes in model predictions compared to expected outcomes, indicating the need for retraining or adjustments.\n",
    "   - Feedback Loops: Capture user feedback and use it to improve model predictions or update training data.\n",
    "\n",
    "3. Deployment Techniques:\n",
    "   - Containerization: Use Docker to encapsulate the model, libraries, and dependencies for consistency across different environments.\n",
    "   - Serverless Computing: Deploy models as functions that automatically scale based on demand (e.g., AWS Lambda, Azure Functions).\n",
    "   - API Integration: Expose models through APIs (Application Programming Interfaces) for easy access and integration with other applications.\n",
    "\n",
    "#### Implementation Steps\n",
    "\n",
    "1. Model Export: Serialize trained models into a format compatible with deployment (e.g., pickle for Python, PMML, ONNX).\n",
    "\n",
    "2. Containerization: Package the model and its dependencies into a Docker container for portability and consistency.\n",
    "\n",
    "3. API Development: Develop an API endpoint using frameworks like Flask or FastAPI to serve model predictions over HTTP.\n",
    "\n",
    "4. Deployment: Deploy the containerized model to a cloud platform (e.g., AWS, Azure, Google Cloud) or on-premises infrastructure.\n",
    "\n",
    "5. Monitoring Setup: Implement monitoring tools and dashboards to track model performance metrics, data drift, and model drift.\n",
    "\n",
    "#### Example: Deploying a Machine Learning Model with Flask\n",
    "\n",
    "Let's deploy a simple machine learning model using Flask, a lightweight web framework for Python, and expose it through an API endpoint.\n",
    "\n",
    "```python\n",
    "# Assuming you have a trained model saved as a pickle file\n",
    "import pickle\n",
    "from flask import Flask, request, jsonify\n",
    "\n",
    "# Load the trained model\n",
    "with open('model.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "# Initialize Flask application\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Define API endpoint for model prediction\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    # Get input data from request\n",
    "    input_data = request.json  # Assuming JSON input format\n",
    "    features = input_data['features']  # Extract features from input\n",
    "\n",
    "    # Perform prediction using the loaded model\n",
    "    prediction = model.predict([features])[0]  # Assuming single prediction\n",
    "\n",
    "    # Prepare response in JSON format\n",
    "    response = {'prediction': prediction}\n",
    "\n",
    "    return jsonify(response)\n",
    "\n",
    "# Run the Flask application\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)\n",
    "```\n",
    "\n",
    "#### Explanation:\n",
    "\n",
    "1. Model Loading: Load a trained model (saved as model.pkl) using pickle.\n",
    "\n",
    "2. Flask Application: Define a Flask application and create an endpoint (/predict) that accepts POST requests with input data.\n",
    "\n",
    "3. Prediction: Receive input data, perform model prediction, and return the prediction as a JSON response.\n",
    "\n",
    "4. Deployment: Run the Flask application, which starts a web server locally. For production, deploy the Flask app to a cloud platform.\n",
    "\n",
    "#### Monitoring and Maintenance\n",
    "\n",
    "- Monitoring Tools: Use tools like Prometheus, Grafana, or custom dashboards to monitor API performance, request latency, and error rates.\n",
    "  \n",
    "- Alerting: Set up alerts for anomalies in model predictions, data drift, or infrastructure issues.\n",
    "\n",
    "- Logging: Implement logging to record API requests, responses, and errors for troubleshooting and auditing purposes.\n",
    "\n",
    "#### Advantages\n",
    "\n",
    "- Scalability: Easily scale models to handle varying workloads and user demands.\n",
    "- Integration: Seamlessly integrate models into existing applications and systems through APIs.\n",
    "- Continuous Improvement: Monitor and update models based on real-world performance and user feedback.\n",
    "\n",
    "Effective deployment and monitoring ensure that machine learning models deliver accurate predictions in production environments, contributing to business success and decision-making.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc009a25-024c-48f9-95a0-76ce42d93bb6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Day 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b74fa96-db2e-4b7b-ac3c-78da0b6807c6",
   "metadata": {},
   "source": [
    "Let's start with Day 30 today \n",
    "\n",
    "30 Days of Data Science Series: https://t.me/datasciencefun/1708\n",
    "\n",
    "Let's learn about Certainly! Let's dive into Hyperparameter Optimization for Day 30 of your data science and machine learning journey.\n",
    "\n",
    "### Day 30: Hyperparameter Optimization\n",
    "\n",
    "#### Concept\n",
    "\n",
    "Hyperparameter optimization involves finding the best set of hyperparameters for a machine learning model to maximize its performance. Hyperparameters are parameters set before the learning process begins, affecting the learning algorithm's behavior and model performance.\n",
    "\n",
    "#### Key Aspects\n",
    "\n",
    "1. Hyperparameters vs. Parameters:\n",
    "   - Parameters: Learned from data during model training (e.g., weights in neural networks).\n",
    "   - Hyperparameters: Set before training and control the learning process (e.g., learning rate, number of trees in a random forest).\n",
    "\n",
    "2. Importance of Hyperparameter Tuning:\n",
    "   - Impact on Model Performance: Proper tuning can significantly improve model accuracy and generalization.\n",
    "   - Algorithm Sensitivity: Different algorithms require different hyperparameters for optimal performance.\n",
    "\n",
    "3. Hyperparameter Optimization Techniques:\n",
    "   - Grid Search: Exhaustively search a predefined grid of hyperparameter values.\n",
    "   - Random Search: Randomly sample hyperparameter combinations from a predefined distribution.\n",
    "   - Bayesian Optimization: Uses probabilistic models to predict the performance of hyperparameter configurations.\n",
    "   - Gradient-based Optimization: Optimizes hyperparameters using gradients derived from the model's performance.\n",
    "\n",
    "4. Evaluation Metrics:\n",
    "   - Cross-Validation: Assess model performance by splitting the data into multiple subsets (folds).\n",
    "   - Scoring Metrics: Use metrics like accuracy, precision, recall, F1-score, or area under the ROC curve (AUC) to evaluate model performance.\n",
    "\n",
    "#### Implementation Steps\n",
    "\n",
    "1. Define Hyperparameters: Identify which hyperparameters need tuning for your specific model and algorithm.\n",
    "\n",
    "2. Choose Optimization Technique: Select an appropriate technique based on computational resources and model complexity.\n",
    "\n",
    "3. Search Space: Define the range or values for each hyperparameter to explore during optimization.\n",
    "\n",
    "4. Evaluation: Evaluate each combination of hyperparameters using cross-validation and chosen evaluation metrics.\n",
    "\n",
    "5. Select Best Model: Choose the model with the best performance based on the evaluation metrics.\n",
    "\n",
    "#### Example: Hyperparameter Tuning with Random Search\n",
    "\n",
    "Let's perform hyperparameter tuning using random search for a Random Forest classifier using scikit-learn.\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import randint\n",
    "\n",
    "# Load dataset\n",
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "\n",
    "# Define model and hyperparameter search space\n",
    "model = RandomForestClassifier()\n",
    "param_dist = {\n",
    "    'n_estimators': randint(10, 200),\n",
    "    'max_depth': randint(5, 50),\n",
    "    'min_samples_split': randint(2, 20),\n",
    "    'min_samples_leaf': randint(1, 20),\n",
    "    'max_features': ['sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "# Randomized search with cross-validation\n",
    "random_search = RandomizedSearchCV(model, param_distributions=param_dist, n_iter=100, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "random_search.fit(X, y)\n",
    "\n",
    "# Print best hyperparameters and score\n",
    "print(\"Best Hyperparameters found:\")\n",
    "print(random_search.best_params_)\n",
    "print(\"Best Accuracy Score found:\")\n",
    "print(random_search.best_score_)\n",
    "```\n",
    "\n",
    "#### Explanation:\n",
    "\n",
    "1. Model and Dataset: We use a RandomForestClassifier on the digits dataset from scikit-learn.\n",
    "\n",
    "2. Hyperparameter Search Space: Defined using param_dist, specifying ranges for n_estimators, max_depth, min_samples_split, min_samples_leaf, and max_features.\n",
    "\n",
    "3. RandomizedSearchCV: Performs random search cross-validation with 5 folds (cv=5) and evaluates models based on accuracy (scoring='accuracy'). n_iter controls the number of random combinations to try.\n",
    "\n",
    "4. Best Parameters: Prints the best hyperparameters (best_params_) and corresponding best accuracy score (best_score_).\n",
    "\n",
    "#### Advantages\n",
    "\n",
    "- Improved Model Performance: Optimal hyperparameters lead to better model accuracy and generalization.\n",
    "  \n",
    "- Efficient Exploration: Techniques like random search and Bayesian optimization efficiently explore the hyperparameter space compared to exhaustive methods.\n",
    "\n",
    "- Flexibility: Hyperparameter tuning is adaptable across different machine learning algorithms and problem domains.\n",
    "\n",
    "#### Conclusion\n",
    "\n",
    "Hyperparameter optimization is crucial for fine-tuning machine learning models to achieve optimal performance. By systematically exploring and evaluating different hyperparameter configurations, data scientists can enhance model accuracy and effectiveness in real-world applications.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c6415d-a640-4adc-892c-9163dd433e3d",
   "metadata": {},
   "source": [
    "# â Statistics & Probability Cheatsheet ðð§ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790ef8c7-b78b-40f5-a778-ac50db4f17a8",
   "metadata": {},
   "source": [
    "### ð Descriptive Statistics:\n",
    "- $Mean = \\frac{\\Sigma x}{n}$\n",
    "- $Median =$ Middle value  \n",
    "- $Mode =$ Most frequent value  \n",
    "- $Variance\\ (\\sigma^2) = \\frac{\\Sigma (x - \\mu)^2}{n}$  \n",
    "- $Std\\ Dev\\ (\\sigma) = \\sqrt{Variance}$  \n",
    "- $Range = Max - Min$  \n",
    "- $IQR = Q3 - Q1$  \n",
    "\n",
    "---\n",
    "\n",
    "### ð Probability Basics:\n",
    "- $P(A) = \\frac{\\text{Outcomes A}}{\\text{Total Outcomes}}$  \n",
    "- $P(A \\cap B) = P(A) \\times P(B)$ (if independent)  \n",
    "- $P(A \\cup B) = P(A) + P(B) - P(A \\cap B)$  \n",
    "- Conditional: $P(A|B) = \\frac{P(A \\cap B)}{P(B)}$  \n",
    "- Bayesâ Theorem: $P(A|B) = \\frac{P(B|A) \\times P(A)}{P(B)}$  \n",
    "\n",
    "---\n",
    "\n",
    "### ð Common Distributions:\n",
    "- Binomial (fixed trials)  \n",
    "- Normal (bell curve)  \n",
    "- Poisson (rare events over time)  \n",
    "- Uniform (equal probability)  \n",
    "\n",
    "---\n",
    "\n",
    "### ð Inferential Stats:\n",
    "- $Z\\text{-score} = \\frac{x - \\mu}{\\sigma}$  \n",
    "- Central Limit Theorem: sampling dist â Normal  \n",
    "- Confidence Interval: $CI = x \\pm z \\left(\\frac{\\sigma}{\\sqrt{n}}\\right)$  \n",
    "\n",
    "---\n",
    "\n",
    "### ð Hypothesis Testing:\n",
    "- $H_0 =$ No effect ; $H_1 =$ Effect present  \n",
    "- p-value < $\\alpha$ â Reject $H_0$  \n",
    "- Tests: t-test (small samples), z-test (known $\\sigma$), chi-square (categorical data)  \n",
    "\n",
    "---\n",
    "\n",
    "### ð Correlation:\n",
    "- Pearson: linear relation (â1 to 1)  \n",
    "- Spearman: rank-based correlation  \n",
    "\n",
    "---\n",
    "\n",
    "### ð§ª Tools to Practice:  \n",
    "Python packages: `scipy.stats`, `statsmodels`, `pandas`  \n",
    "Visualization: `seaborn`, `matplotlib`  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b33a06-7ca6-4444-8505-dc8c779e93b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
